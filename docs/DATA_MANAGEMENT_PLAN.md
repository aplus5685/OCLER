# ì˜¤í´ëŸ¬ í”„ë¡œì íŠ¸ ë°ì´í„° ê´€ë¦¬ ê³„íšì„œ

**ì‘ì„±ì**: íŒ€ì¥í‚´(Project Manager) & ê°•ë””ë¹„(Database Optimizer)  
**ì‘ì„±ì¼**: 2025-07-30  
**ë²„ì „**: v1.0  

---

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### í˜„ì¬ ìƒí™© ë¶„ì„
- **ë°ì´í„° ê·œëª¨**: 12,000ê°œ ì—…ì²´ ì •ë³´ (CSV í˜•ì‹)
- **ë°ì´í„° êµ¬ì„±**: ì—…ì²´ëª…, ìœ„ì¹˜ì •ë³´, ì¹´í…Œê³ ë¦¬, ìš´ì˜ìƒíƒœ ë“± 23ê°œ í•„ë“œ
- **í™œìš© ëª©ì **: ì˜¤í´ëŸ¬ ì•±ì—ì„œ ì—…ì²´ ê²€ìƒ‰ ë° ì •ë³´ ì œê³µ
- **ìƒíƒœ ì •ë³´**: ì˜ì—…ì¤‘/íì—…/ë¹ˆì í¬ ìƒíƒœ ì¶”ì 

### í”„ë¡œì íŠ¸ ëª©í‘œ
1. ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
2. ì‹¤ì‹œê°„ ì—…ì²´ ì •ë³´ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ êµ¬í˜„
3. ë°ì´í„° í’ˆì§ˆ ë³´ì¥ ë° ì¼ê´€ì„± ìœ ì§€
4. ê°œë°œíŒ€ ì¹œí™”ì ì¸ ë°ì´í„° ì ‘ê·¼ ë°©ë²• ì œê³µ

---

## 1. ë°ì´í„° êµ¬ì¡° ë¶„ì„ ë° ì •ê·œí™” ë°©ì•ˆ

### 1.1 í˜„ì¬ CSV ë°ì´í„° êµ¬ì¡° ë¶„ì„

```csv
ì»¬ëŸ¼ëª…: id, store_key, name, hotplace, address_a, address_b, address_c, 
        category_a, category_b, keywords_a, keywords_b, keywords_c, 
        map_link, open_date, close_date, status, concept, extra_link, 
        description, thumbnail, photos, parking, event
```

### 1.2 ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë¶„ì„

**ğŸ” íŒ€ì¥í‚´ì˜ ë¶„ì„**:
- **ì¤‘ë³µ ë°ì´í„°**: store_keyê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ë‹¤ìˆ˜ ë°œê²¬
- **ëˆ„ë½ ë°ì´í„°**: status í•„ë“œê°€ ëŒ€ë¶€ë¶„ NULL
- **ë¹„ì •ê·œí™”**: ì£¼ì†Œ ì •ë³´ê°€ 3ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ì‚°
- **ì¼ê´€ì„± ë¶€ì¡±**: ë‚ ì§œ í˜•ì‹ì´ "2021-Q1" í˜•íƒœë¡œ ë¹„í‘œì¤€

**âš¡ ê°•ë””ë¹„ì˜ ìµœì í™” ì œì•ˆ**:
```sql
-- ë°ì´í„° ì •ê·œí™”ë¥¼ ìœ„í•œ ì°¸ì¡° í…Œì´ë¸” êµ¬ì¡°
-- 1) ì§€ì—­ ì •ë³´ ì •ê·œí™”
CREATE TABLE regions (
    id UUID PRIMARY KEY,
    hotplace_code VARCHAR(10) UNIQUE,
    name VARCHAR(50),
    district VARCHAR(50),
    full_name VARCHAR(100)
);

-- 2) ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡° ì •ê·œí™”
CREATE TABLE categories (
    id UUID PRIMARY KEY,
    code VARCHAR(20) UNIQUE,
    name VARCHAR(100),
    parent_id UUID REFERENCES categories(id),
    level INTEGER CHECK (level IN (1, 2))
);

-- 3) í‚¤ì›Œë“œ ì •ê·œí™” (ë‹¤ëŒ€ë‹¤ ê´€ê³„)
CREATE TABLE business_keywords (
    business_id UUID REFERENCES businesses(id),
    keyword VARCHAR(100),
    keyword_type VARCHAR(20) CHECK (keyword_type IN ('primary', 'secondary', 'tertiary')),
    PRIMARY KEY (business_id, keyword)
);
```

### 1.3 ì •ê·œí™” ì „ëµ

```mermaid
graph TD
    A[CSV Raw Data] --> B[ë°ì´í„° ê²€ì¦]
    B --> C[ì°¸ì¡° í…Œì´ë¸” ìƒì„±]
    C --> D[ë©”ì¸ ë°ì´í„° ì •ê·œí™”]
    D --> E[ê´€ê³„ ì„¤ì •]
    E --> F[ì¸ë±ìŠ¤ ìµœì í™”]
```

---

## 2. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ (í…Œì´ë¸” êµ¬ì¡°)

### 2.1 ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°

**ğŸ¯ íŒ€ì¥í‚´ì˜ ìš”êµ¬ì‚¬í•­**:
- ë¹ ë¥¸ ê²€ìƒ‰ ì„±ëŠ¥
- í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°
- ê°œë°œíŒ€ ì¹œí™”ì  API

**âš¡ ê°•ë””ë¹„ì˜ ì„±ëŠ¥ ìµœì í™” ì„¤ê³„**:

```sql
-- í•µì‹¬ ì—…ì²´ í…Œì´ë¸” (ìµœì í™”ëœ êµ¬ì¡°)
CREATE TABLE businesses (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    
    -- ê¸°ë³¸ ì •ë³´
    original_id VARCHAR(20) UNIQUE NOT NULL,
    store_key VARCHAR(50),
    name VARCHAR(200) NOT NULL,
    
    -- ìœ„ì¹˜ ì •ë³´ (ì •ê·œí™”)
    hotplace_id UUID REFERENCES hotplaces(id) NOT NULL,
    address_detail VARCHAR(200),
    latitude DECIMAL(10, 7),
    longitude DECIMAL(10, 7),
    
    -- ì¹´í…Œê³ ë¦¬ (ì •ê·œí™”)
    category_id UUID REFERENCES categories(id) NOT NULL,
    subcategory_id UUID REFERENCES subcategories(id),
    
    -- ê²€ìƒ‰ ìµœì í™” (GIN ì¸ë±ìŠ¤ìš©)
    keywords JSONB DEFAULT '[]'::jsonb,
    search_vector TSVECTOR GENERATED ALWAYS AS (
        to_tsvector('korean', 
            COALESCE(name, '') || ' ' || 
            COALESCE(array_to_string(ARRAY(SELECT jsonb_array_elements_text(keywords)), ' '), '')
        )
    ) STORED,
    
    -- ìš´ì˜ ìƒíƒœ ê´€ë¦¬ (ê°œì„ ëœ ë¡œì§)
    status business_status DEFAULT 'active',  -- ENUM íƒ€ì…
    open_date DATE,
    close_date DATE,
    last_verified_at TIMESTAMP WITH TIME ZONE,
    
    -- ë¯¸ë””ì–´ ë° ë¶€ê°€ ì •ë³´
    thumbnail TEXT,
    photos JSONB DEFAULT '[]'::jsonb,
    description TEXT,
    concept TEXT,
    
    -- ì™¸ë¶€ ë§í¬
    map_link TEXT,
    website_url TEXT,
    
    -- í¸ì˜ì‹œì„¤ (ë¹„íŠ¸ë§ˆìŠ¤í¬ ë°©ì‹ìœ¼ë¡œ ìµœì í™”)
    amenities INTEGER DEFAULT 0,  -- ì£¼ì°¨, ì´ë²¤íŠ¸ ë“±ì„ ë¹„íŠ¸ë¡œ ê´€ë¦¬
    
    -- ë¶„ì„ ë°ì´í„°
    view_count INTEGER DEFAULT 0,
    search_count INTEGER DEFAULT 0,
    favorite_count INTEGER DEFAULT 0,
    rating_avg DECIMAL(3,2) DEFAULT 0.0,
    
    -- ë©”íƒ€ë°ì´í„°
    data_source VARCHAR(50) DEFAULT 'csv_import',
    data_quality_score INTEGER DEFAULT 0,  -- 0-100 ë°ì´í„° í’ˆì§ˆ ì ìˆ˜
    raw_data JSONB,  -- ì›ë³¸ ë°ì´í„° ë³´ê´€
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì—…ì²´ ìƒíƒœ ENUM ì •ì˜
CREATE TYPE business_status AS ENUM ('active', 'closed', 'temporary_closed', 'pending_verification');

-- í¸ì˜ì‹œì„¤ ë¹„íŠ¸ë§ˆìŠ¤í¬ ìƒìˆ˜
/*
í¸ì˜ì‹œì„¤ ë¹„íŠ¸ ì •ì˜:
BIT 0 (1): ì£¼ì°¨ ê°€ëŠ¥
BIT 1 (2): ì´ë²¤íŠ¸ ì§„í–‰
BIT 2 (4): ë°°ë‹¬ ê°€ëŠ¥
BIT 3 (8): í¬ì¥ ê°€ëŠ¥
BIT 4 (16): ì˜ˆì•½ ê°€ëŠ¥
BIT 5 (32): ì¹´ë“œ ê²°ì œ ê°€ëŠ¥
*/
```

### 2.2 ê³ ì„±ëŠ¥ ì¸ë±ìŠ¤ ì „ëµ

```sql
-- ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_businesses_search_gin 
ON businesses USING GIN(search_vector);

CREATE INDEX CONCURRENTLY idx_businesses_location_btree 
ON businesses(hotplace_id, category_id) WHERE status = 'active';

CREATE INDEX CONCURRENTLY idx_businesses_compound_search 
ON businesses(status, hotplace_id, category_id) 
INCLUDE (name, view_count);

-- ë¶„ì„ ì¿¼ë¦¬ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_businesses_analytics 
ON businesses(created_at, status) 
WHERE data_quality_score > 50;

-- ì§€ë¦¬ì  ê²€ìƒ‰ ìµœì í™” (í–¥í›„ í™•ì¥ìš©)
CREATE INDEX CONCURRENTLY idx_businesses_geo 
ON businesses USING GIST(ST_Point(longitude, latitude));
```

### 2.3 íŒŒí‹°ì…”ë‹ ì „ëµ (í™•ì¥ì„± ê³ ë ¤)

```sql
-- ë¶„ì„ ë°ì´í„° ì›”ë³„ íŒŒí‹°ì…”ë‹
CREATE TABLE business_analytics (
    id UUID DEFAULT gen_random_uuid(),
    business_id UUID NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    event_date DATE NOT NULL DEFAULT CURRENT_DATE,
    count INTEGER DEFAULT 1,
    metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
) PARTITION BY RANGE (event_date);

-- ì›”ë³„ íŒŒí‹°ì…˜ ìƒì„± ì˜ˆì‹œ
CREATE TABLE business_analytics_2025_07 PARTITION OF business_analytics
FOR VALUES FROM ('2025-07-01') TO ('2025-08-01');

CREATE TABLE business_analytics_2025_08 PARTITION OF business_analytics  
FOR VALUES FROM ('2025-08-01') TO ('2025-09-01');
```

---

## 3. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš (ì—‘ì…€ â†’ DB)

### 3.1 ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

**ğŸ¯ íŒ€ì¥í‚´ì˜ í”„ë¡œì íŠ¸ ê´€ë¦¬ ê´€ì **:
1. **ë‹¨ê³„ë³„ ì§„í–‰**: í…ŒìŠ¤íŠ¸ â†’ ìŠ¤í…Œì´ì§• â†’ í”„ë¡œë•ì…˜
2. **ë°±ì—… ìš°ì„ **: ì›ë³¸ ë°ì´í„° ë³´ì¡´
3. **ê²€ì¦ í•„ìˆ˜**: ê° ë‹¨ê³„ë³„ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
4. **ë¡¤ë°± ê³„íš**: ë¬¸ì œ ë°œìƒì‹œ ì¦‰ì‹œ ë³µêµ¬

### 3.2 ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

```python
# migration_manager.py
import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor
import uuid
import json
import logging
from datetime import datetime, date
import re

class DataMigrationManager:
    def __init__(self, db_config):
        self.db_config = db_config
        self.connection = None
        self.logger = self._setup_logger()
        
    def _setup_logger(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'migration_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.connection = psycopg2.connect(**self.db_config)
            self.connection.autocommit = False
            self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def load_csv_data(self, csv_path):
        """CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            # CSV ë¡œë“œ (ì¸ì½”ë”© ìë™ ê°ì§€)
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            self.logger.info(f"CSV ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            df = self._preprocess_data(df)
            
            return df
        except Exception as e:
            self.logger.error(f"CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _preprocess_data(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° í’ˆì§ˆ ê°œì„ """
        original_count = len(df)
        
        # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
        df = df.dropna(subset=['id', 'name'])
        
        # 2. ë‚ ì§œ í˜•ì‹ ì •ê·œí™” (2021-Q1 â†’ 2021-01-01)
        df['open_date_parsed'] = df['open_date'].apply(self._parse_quarter_date)
        df['close_date_parsed'] = df['close_date'].apply(self._parse_quarter_date)
        
        # 3. ìƒíƒœ ì •ë³´ ì •ê·œí™”
        df['status_normalized'] = df.apply(self._normalize_status, axis=1)
        
        # 4. í‚¤ì›Œë“œ JSON ë°°ì—´ë¡œ ë³€í™˜
        df['keywords_json'] = df.apply(self._combine_keywords, axis=1)
        
        # 5. ì£¼ì†Œ ì •ë³´ ê²°í•©
        df['full_address'] = df['address_a'].fillna('') + ' ' + \
                           df['address_b'].fillna('') + ' ' + \
                           df['address_c'].fillna('')
        
        # 6. ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        df['quality_score'] = df.apply(self._calculate_quality_score, axis=1)
        
        cleaned_count = len(df)
        self.logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {original_count} â†’ {cleaned_count} í–‰")
        
        return df
    
    def _parse_quarter_date(self, quarter_str):
        """ë¶„ê¸° í˜•ì‹ ë‚ ì§œë¥¼ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜"""
        if pd.isna(quarter_str) or quarter_str == '-':
            return None
        
        pattern = r'(\d{4})-Q([1-4])'
        match = re.match(pattern, str(quarter_str))
        
        if match:
            year, quarter = match.groups()
            quarter_months = {'1': '01', '2': '04', '3': '07', '4': '10'}
            return f"{year}-{quarter_months[quarter]}-01"
        
        return None
    
    def _normalize_status(self, row):
        """ì—…ì²´ ìƒíƒœ ì •ê·œí™”"""
        if pd.notna(row['close_date']) and row['close_date'] != '-':
            return 'closed'
        elif row['status'] == 'NULL' or pd.isna(row['status']):
            return 'active'  # ê¸°ë³¸ê°’
        else:
            return 'active'
    
    def _combine_keywords(self, row):
        """í‚¤ì›Œë“œë¥¼ JSON ë°°ì—´ë¡œ ê²°í•©"""
        keywords = []
        for col in ['keywords_a', 'keywords_b', 'keywords_c']:
            if pd.notna(row[col]) and row[col].strip():
                keywords.append(row[col].strip())
        return json.dumps(keywords, ensure_ascii=False)
    
    def _calculate_quality_score(self, row):
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        score = 0
        
        # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ (40ì )
        if pd.notna(row['name']) and row['name'].strip():
            score += 20
        if pd.notna(row['hotplace']) and row['hotplace'].strip():
            score += 10
        if pd.notna(row['category_a']) and row['category_a'].strip():
            score += 10
        
        # ì„ íƒ í•„ë“œ ì¡´ì¬ (30ì )
        if pd.notna(row['map_link']) and row['map_link'].strip():
            score += 15
        if pd.notna(row['description']) and row['description'].strip():
            score += 10
        if pd.notna(row['keywords_a']) and row['keywords_a'].strip():
            score += 5
        
        # ì£¼ì†Œ ì •ë³´ ì™„ì„±ë„ (20ì )
        address_fields = ['address_a', 'address_b', 'address_c']
        filled_fields = sum(1 for field in address_fields 
                          if pd.notna(row[field]) and row[field].strip())
        score += (filled_fields / len(address_fields)) * 20
        
        # ë‚ ì§œ ì •ë³´ ì¡´ì¬ (10ì )
        if pd.notna(row['open_date']) and row['open_date'] != '-':
            score += 10
        
        return int(score)
    
    def create_reference_tables(self, df):
        """ì°¸ì¡° í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ì‚½ì…"""
        cursor = self.connection.cursor(cursor_factory=RealDictCursor)
        
        try:
            # 1. í•«í”Œë ˆì´ìŠ¤ í…Œì´ë¸” ìƒì„±
            hotplaces = df['hotplace'].dropna().unique()
            for hotplace in hotplaces:
                cursor.execute("""
                    INSERT INTO hotplaces (name, display_name, district)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (name) DO NOTHING
                """, (hotplace, hotplace, hotplace))
            
            # 2. ì¹´í…Œê³ ë¦¬ í…Œì´ë¸” ìƒì„±
            categories = df['category_a'].dropna().unique()
            for category in categories:
                cursor.execute("""
                    INSERT INTO categories (name, display_name)
                    VALUES (%s, %s)
                    ON CONFLICT (name) DO NOTHING
                """, (category, category))
            
            # 3. ì„œë¸Œì¹´í…Œê³ ë¦¬ í…Œì´ë¸” ìƒì„±
            for _, row in df[['category_a', 'category_b']].dropna().drop_duplicates().iterrows():
                cursor.execute("""
                    INSERT INTO subcategories (category_id, name, display_name)
                    SELECT c.id, %s, %s
                    FROM categories c
                    WHERE c.name = %s
                    ON CONFLICT DO NOTHING
                """, (row['category_b'], row['category_b'], row['category_a']))
            
            self.connection.commit()
            self.logger.info("ì°¸ì¡° í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.connection.rollback()
            self.logger.error(f"ì°¸ì¡° í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            raise
        finally:
            cursor.close()
    
    def migrate_businesses(self, df):
        """ì—…ì²´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        cursor = self.connection.cursor(cursor_factory=RealDictCursor)
        
        success_count = 0
        error_count = 0
        
        try:
            for index, row in df.iterrows():
                try:
                    # ì°¸ì¡° ID ì¡°íšŒ
                    hotplace_id = self._get_reference_id(cursor, 'hotplaces', 'name', row['hotplace'])
                    category_id = self._get_reference_id(cursor, 'categories', 'name', row['category_a'])
                    subcategory_id = None
                    if pd.notna(row['category_b']):
                        subcategory_id = self._get_subcategory_id(cursor, row['category_a'], row['category_b'])
                    
                    # ì—…ì²´ ë°ì´í„° ì‚½ì…
                    cursor.execute("""
                        INSERT INTO businesses (
                            original_id, store_key, name, hotplace_id, category_id, subcategory_id,
                            address_detail, keywords, open_date, close_date, status,
                            map_link, description, concept, thumbnail, 
                            has_parking, has_event, data_quality_score, raw_data
                        ) VALUES (
                            %s, %s, %s, %s, %s, %s, %s, %s::jsonb, %s, %s, %s,
                            %s, %s, %s, %s, %s, %s, %s, %s::jsonb
                        )
                        ON CONFLICT (original_id) DO UPDATE SET
                            name = EXCLUDED.name,
                            updated_at = NOW()
                    """, (
                        row['id'], row['store_key'], row['name'], hotplace_id, 
                        category_id, subcategory_id, row['full_address'], 
                        row['keywords_json'], row['open_date_parsed'], 
                        row['close_date_parsed'], row['status_normalized'],
                        row['map_link'], row['description'], row['concept'], 
                        row['thumbnail'], 
                        bool(row['parking']) if pd.notna(row['parking']) else False,
                        bool(row['event']) if pd.notna(row['event']) else False,
                        row['quality_score'], json.dumps(row.to_dict(), default=str, ensure_ascii=False)
                    ))
                    
                    success_count += 1
                    
                    if success_count % 100 == 0:
                        self.connection.commit()
                        self.logger.info(f"ì§„í–‰ ìƒí™©: {success_count}/{len(df)} ì™„ë£Œ")
                
                except Exception as e:
                    error_count += 1
                    self.logger.error(f"í–‰ {index} ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
                    continue
            
            self.connection.commit()
            self.logger.info(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {error_count}")
            
        except Exception as e:
            self.connection.rollback()
            self.logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            raise
        finally:
            cursor.close()
    
    def _get_reference_id(self, cursor, table, column, value):
        """ì°¸ì¡° í…Œì´ë¸” ID ì¡°íšŒ"""
        cursor.execute(f"SELECT id FROM {table} WHERE {column} = %s", (value,))
        result = cursor.fetchone()
        return result['id'] if result else None
    
    def _get_subcategory_id(self, cursor, category_name, subcategory_name):
        """ì„œë¸Œì¹´í…Œê³ ë¦¬ ID ì¡°íšŒ"""
        cursor.execute("""
            SELECT sc.id 
            FROM subcategories sc
            JOIN categories c ON sc.category_id = c.id
            WHERE c.name = %s AND sc.name = %s
        """, (category_name, subcategory_name))
        result = cursor.fetchone()
        return result['id'] if result else None
    
    def validate_migration(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦"""
        cursor = self.connection.cursor(cursor_factory=RealDictCursor)
        
        try:
            # 1. ì „ì²´ ë°ì´í„° ìˆ˜ í™•ì¸
            cursor.execute("SELECT COUNT(*) as count FROM businesses")
            business_count = cursor.fetchone()['count']
            
            # 2. ë°ì´í„° í’ˆì§ˆ ë¶„í¬ í™•ì¸
            cursor.execute("""
                SELECT 
                    AVG(data_quality_score) as avg_quality,
                    MIN(data_quality_score) as min_quality,
                    MAX(data_quality_score) as max_quality,
                    COUNT(CASE WHEN data_quality_score >= 80 THEN 1 END) as high_quality_count
                FROM businesses
            """)
            quality_stats = cursor.fetchone()
            
            # 3. ìƒíƒœë³„ ë¶„í¬ í™•ì¸
            cursor.execute("""
                SELECT status, COUNT(*) as count 
                FROM businesses 
                GROUP BY status
            """)
            status_stats = cursor.fetchall()
            
            # 4. ì°¸ì¡° ë¬´ê²°ì„± í™•ì¸
            cursor.execute("""
                SELECT COUNT(*) as orphaned_businesses
                FROM businesses b
                LEFT JOIN hotplaces h ON b.hotplace_id = h.id
                LEFT JOIN categories c ON b.category_id = c.id
                WHERE h.id IS NULL OR c.id IS NULL
            """)
            orphaned_count = cursor.fetchone()['orphaned_count']
            
            # ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸
            report = {
                'total_businesses': business_count,
                'quality_stats': dict(quality_stats),
                'status_distribution': {row['status']: row['count'] for row in status_stats},
                'orphaned_records': orphaned_count,
                'validation_passed': orphaned_count == 0
            }
            
            self.logger.info("=== ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ê²°ê³¼ ===")
            self.logger.info(json.dumps(report, indent=2, ensure_ascii=False))
            
            return report
            
        except Exception as e:
            self.logger.error(f"ê²€ì¦ ì‹¤íŒ¨: {e}")
            raise
        finally:
            cursor.close()
    
    def close_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.connection:
            self.connection.close()
            self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")

# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
if __name__ == "__main__":
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    DB_CONFIG = {
        'host': 'localhost',
        'database': 'ocler_db',
        'user': 'ocler_user',
        'password': 'secure_password',
        'port': 5432
    }
    
    # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    migrator = DataMigrationManager(DB_CONFIG)
    
    try:
        # 1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        migrator.connect_db()
        
        # 2. CSV ë°ì´í„° ë¡œë“œ
        df = migrator.load_csv_data('C:/dev/OCLER/raw data-list-cvs.csv')
        
        # 3. ì°¸ì¡° í…Œì´ë¸” ìƒì„±
        migrator.create_reference_tables(df)
        
        # 4. ì—…ì²´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
        migrator.migrate_businesses(df)
        
        # 5. ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦
        validation_result = migrator.validate_migration()
        
        if validation_result['validation_passed']:
            print("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print("âš ï¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ê²€ì¦ ì¤‘ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
    finally:
        migrator.close_connection()
```

### 3.3 ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
# batch_processor.py
class BatchProcessor:
    def __init__(self, batch_size=1000):
        self.batch_size = batch_size
    
    def process_in_batches(self, data, process_func):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
        total_batches = len(data) // self.batch_size + 1
        
        for i in range(0, len(data), self.batch_size):
            batch = data[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            
            print(f"ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            
            try:
                process_func(batch)
            except Exception as e:
                print(f"ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ë¥¼ ê°œë³„ ì²˜ë¦¬ë¡œ ì¬ì‹œë„
                for item in batch:
                    try:
                        process_func([item])
                    except:
                        continue
```

---

## 4. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ë° ê²€ì¦ ë°©ë²•

### 4.1 ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ í”„ë ˆì„ì›Œí¬

**ğŸ¯ íŒ€ì¥í‚´ì˜ í’ˆì§ˆ ê´€ë¦¬ ì „ëµ**:
1. **ìë™í™”ëœ ê²€ì¦**: ë°ì´í„° ì…ë ¥ì‹œ ì‹¤ì‹œê°„ ê²€ì¦
2. **ì •ê¸°ì  ê°ì‚¬**: ì£¼ê¸°ì  ë°ì´í„° í’ˆì§ˆ ì ê²€
3. **ì´ìƒ íƒì§€**: ë¹„ì •ìƒì ì¸ ë°ì´í„° íŒ¨í„´ ê°ì§€
4. **í’ˆì§ˆ ì ìˆ˜**: ê° ë ˆì½”ë“œë³„ í’ˆì§ˆ ì§€í‘œ ê´€ë¦¬

```python
# data_quality_manager.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re
from typing import Dict, List, Tuple

class DataQualityManager:
    def __init__(self, db_connection):
        self.db = db_connection
        self.quality_rules = self._initialize_quality_rules()
        
    def _initialize_quality_rules(self):
        """ë°ì´í„° í’ˆì§ˆ ê·œì¹™ ì •ì˜"""
        return {
            'completeness': {
                'required_fields': ['name', 'hotplace', 'category_a'],
                'recommended_fields': ['map_link', 'description', 'keywords_a'],
                'weight': 0.4
            },
            'validity': {
                'name_pattern': r'^[\w\s\-\.ê°€-í£]+$',
                'url_pattern': r'^https?://.*',
                'date_pattern': r'^\d{4}-\d{2}-\d{2}$',
                'weight': 0.3
            },
            'consistency': {
                'status_close_date_match': True,
                'category_subcategory_match': True,
                'weight': 0.2
            },
            'accuracy': {
                'duplicate_detection': True,
                'business_name_similarity': 0.85,
                'weight': 0.1
            }
        }
    
    def calculate_quality_score(self, business_data: Dict) -> Dict:
        """ì—…ì²´ ë°ì´í„°ì˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        scores = {}
        
        # 1. ì™„ì„±ë„ ì ìˆ˜
        scores['completeness'] = self._check_completeness(business_data)
        
        # 2. ìœ íš¨ì„± ì ìˆ˜
        scores['validity'] = self._check_validity(business_data)
        
        # 3. ì¼ê´€ì„± ì ìˆ˜
        scores['consistency'] = self._check_consistency(business_data)
        
        # 4. ì •í™•ì„± ì ìˆ˜
        scores['accuracy'] = self._check_accuracy(business_data)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_score = sum(
            scores[category] * self.quality_rules[category]['weight']
            for category in scores
        )
        
        return {
            'total_score': round(total_score, 2),
            'category_scores': scores,
            'recommendations': self._generate_recommendations(scores)
        }
    
    def _check_completeness(self, data: Dict) -> float:
        """ë°ì´í„° ì™„ì„±ë„ ê²€ì‚¬"""
        required_score = 0
        required_fields = self.quality_rules['completeness']['required_fields']
        
        for field in required_fields:
            if data.get(field) and str(data[field]).strip():
                required_score += 1
        
        required_ratio = required_score / len(required_fields)
        
        # ê¶Œì¥ í•„ë“œ ê²€ì‚¬
        recommended_score = 0
        recommended_fields = self.quality_rules['completeness']['recommended_fields']
        
        for field in recommended_fields:
            if data.get(field) and str(data[field]).strip():
                recommended_score += 1
        
        recommended_ratio = recommended_score / len(recommended_fields)
        
        # í•„ìˆ˜ 70%, ê¶Œì¥ 30% ë¹„ì¤‘
        return (required_ratio * 0.7 + recommended_ratio * 0.3) * 100
    
    def _check_validity(self, data: Dict) -> float:
        """ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
        validity_checks = [
            self._validate_name(data.get('name')),
            self._validate_url(data.get('map_link')),
            self._validate_date(data.get('open_date')),
            self._validate_date(data.get('close_date')),
            self._validate_status(data.get('status'))
        ]
        
        valid_count = sum(1 for check in validity_checks if check)
        return (valid_count / len(validity_checks)) * 100
    
    def _check_consistency(self, data: Dict) -> float:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬"""
        consistency_score = 100
        
        # íì—… ìƒíƒœì™€ íì—…ì¼ ì¼ì¹˜ì„±
        if data.get('status') == 'closed' and not data.get('close_date'):
            consistency_score -= 30
        
        if data.get('close_date') and data.get('status') != 'closed':
            consistency_score -= 30
        
        # ì¹´í…Œê³ ë¦¬-ì„œë¸Œì¹´í…Œê³ ë¦¬ ì¼ì¹˜ì„±
        if not self._validate_category_subcategory_match(
            data.get('category_a'), data.get('category_b')
        ):
            consistency_score -= 40
        
        return max(0, consistency_score)
    
    def _check_accuracy(self, data: Dict) -> float:
        """ë°ì´í„° ì •í™•ì„± ê²€ì‚¬"""
        accuracy_score = 100
        
        # ì¤‘ë³µ ì—…ì²´ ê²€ì‚¬
        if self._check_duplicate_business(data):
            accuracy_score -= 50
        
        # ì£¼ì†Œ ì •í™•ì„± ê²€ì‚¬
        if not self._validate_address_format(data):
            accuracy_score -= 30
        
        # ì™¸ë¶€ ë§í¬ ì ‘ê·¼ì„± ê²€ì‚¬ (ë¹„ë™ê¸° ì²˜ë¦¬ í•„ìš”)
        if data.get('map_link') and not self._check_url_accessibility(data['map_link']):
            accuracy_score -= 20
        
        return max(0, accuracy_score)
    
    def _validate_name(self, name: str) -> bool:
        """ì—…ì²´ëª… ìœ íš¨ì„± ê²€ì‚¬"""
        if not name:
            return False
        return len(name.strip()) >= 2 and len(name) <= 100
    
    def _validate_url(self, url: str) -> bool:
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        if not url:
            return True  # ì„ íƒ í•„ë“œì´ë¯€ë¡œ ì—†ì–´ë„ ìœ íš¨
        pattern = self.quality_rules['validity']['url_pattern']
        return bool(re.match(pattern, url))
    
    def _validate_date(self, date_str: str) -> bool:
        """ë‚ ì§œ í˜•ì‹ ìœ íš¨ì„± ê²€ì‚¬"""
        if not date_str:
            return True  # ì„ íƒ í•„ë“œ
        try:
            datetime.strptime(date_str, '%Y-%m-%d')
            return True
        except ValueError:
            return False
    
    def _validate_status(self, status: str) -> bool:
        """ìƒíƒœ ê°’ ìœ íš¨ì„± ê²€ì‚¬"""
        valid_statuses = ['active', 'closed', 'temporary_closed', 'pending_verification']
        return status in valid_statuses
    
    def _validate_category_subcategory_match(self, category: str, subcategory: str) -> bool:
        """ì¹´í…Œê³ ë¦¬-ì„œë¸Œì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ê²€ì‚¬"""
        if not category or not subcategory:
            return True
        
        # DBì—ì„œ ìœ íš¨í•œ ì¡°í•©ì¸ì§€ í™•ì¸
        cursor = self.db.cursor()
        cursor.execute("""
            SELECT 1 FROM subcategories sc
            JOIN categories c ON sc.category_id = c.id
            WHERE c.name = %s AND sc.name = %s
        """, (category, subcategory))
        
        return cursor.fetchone() is not None
    
    def _check_duplicate_business(self, data: Dict) -> bool:
        """ì¤‘ë³µ ì—…ì²´ ê²€ì‚¬"""
        cursor = self.db.cursor()
        cursor.execute("""
            SELECT COUNT(*) as count FROM businesses
            WHERE name = %s AND hotplace_id = (
                SELECT id FROM hotplaces WHERE name = %s
            ) AND id != %s
        """, (data.get('name'), data.get('hotplace'), data.get('id', '')))
        
        result = cursor.fetchone()
        return result['count'] > 0
    
    def _validate_address_format(self, data: Dict) -> bool:
        """ì£¼ì†Œ í˜•ì‹ ê²€ì‚¬"""
        address_parts = [
            data.get('address_a'),
            data.get('address_b'), 
            data.get('address_c')
        ]
        
        # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì£¼ì†Œ êµ¬ì„±ìš”ì†Œê°€ ìˆì–´ì•¼ í•¨
        filled_parts = [part for part in address_parts if part and part.strip()]
        return len(filled_parts) >= 2
    
    def _check_url_accessibility(self, url: str) -> bool:
        """URL ì ‘ê·¼ ê°€ëŠ¥ì„± ê²€ì‚¬ (ê°„ë‹¨í•œ í˜•íƒœ)"""
        try:
            import requests
            response = requests.head(url, timeout=5)
            return response.status_code < 400
        except:
            return False
    
    def _generate_recommendations(self, scores: Dict) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if scores['completeness'] < 80:
            recommendations.append("í•„ìˆ˜ ì •ë³´(ì—…ì²´ëª…, ìœ„ì¹˜, ì¹´í…Œê³ ë¦¬)ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        if scores['validity'] < 70:
            recommendations.append("ì…ë ¥ëœ ì •ë³´ì˜ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”. (URL, ë‚ ì§œ í˜•ì‹ ë“±)")
        
        if scores['consistency'] < 80:
            recommendations.append("ì—…ì²´ ìƒíƒœì™€ ê´€ë ¨ ì •ë³´(íì—…ì¼ ë“±)ì˜ ì¼ê´€ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        if scores['accuracy'] < 70:
            recommendations.append("ì¤‘ë³µ ì—…ì²´ê°€ ì—†ëŠ”ì§€, ì£¼ì†Œ ì •ë³´ê°€ ì •í™•í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        return recommendations

    def run_quality_audit(self) -> Dict:
        """ì „ì²´ ë°ì´í„° í’ˆì§ˆ ê°ì‚¬ ì‹¤í–‰"""
        cursor = self.db.cursor()
        
        # ì „ì²´ ì—…ì²´ ë°ì´í„° ì¡°íšŒ
        cursor.execute("""
            SELECT b.*, h.name as hotplace_name, c.name as category_name
            FROM businesses b
            LEFT JOIN hotplaces h ON b.hotplace_id = h.id
            LEFT JOIN categories c ON b.category_id = c.id
        """)
        
        businesses = cursor.fetchall()
        
        audit_results = {
            'total_businesses': len(businesses),
            'quality_distribution': {'high': 0, 'medium': 0, 'low': 0},
            'common_issues': {},
            'recommendations': []
        }
        
        for business in businesses:
            quality_result = self.calculate_quality_score(dict(business))
            score = quality_result['total_score']
            
            # í’ˆì§ˆ ë“±ê¸‰ ë¶„ë¥˜
            if score >= 80:
                audit_results['quality_distribution']['high'] += 1
            elif score >= 60:
                audit_results['quality_distribution']['medium'] += 1
            else:
                audit_results['quality_distribution']['low'] += 1
        
        return audit_results

# ì‚¬ìš© ì˜ˆì‹œ
quality_manager = DataQualityManager(db_connection)

# ê°œë³„ ì—…ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
business_data = {
    'name': 'ì„œë©´ë§›ì§‘',
    'hotplace': 'ì„œë©´',
    'category_a': 'ì‹ì‚¬ìˆ ',
    'category_b': 'í•œì‹.ë¶„ì‹',
    'map_link': 'http://naver.me/example',
    'status': 'active'
}

quality_result = quality_manager.calculate_quality_score(business_data)
print(f"í’ˆì§ˆ ì ìˆ˜: {quality_result['total_score']}/100")
print(f"ê°œì„  ê¶Œì¥ì‚¬í•­: {quality_result['recommendations']}")
```

### 4.2 ì‹¤ì‹œê°„ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ

```sql
-- ë°ì´í„° ê²€ì¦ íŠ¸ë¦¬ê±°
CREATE OR REPLACE FUNCTION validate_business_data()
RETURNS TRIGGER AS $$
BEGIN
    -- 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
    IF NEW.name IS NULL OR LENGTH(TRIM(NEW.name)) < 2 THEN
        RAISE EXCEPTION 'ì—…ì²´ëª…ì€ 2ê¸€ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.';
    END IF;
    
    -- 2. ìƒíƒœ-ë‚ ì§œ ì¼ê´€ì„± ê²€ì¦
    IF NEW.status = 'closed' AND NEW.close_date IS NULL THEN
        RAISE EXCEPTION 'íì—… ìƒíƒœì¸ ê²½ìš° íì—…ì¼ì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.';
    END IF;
    
    -- 3. ë‚ ì§œ ë…¼ë¦¬ ê²€ì¦
    IF NEW.open_date IS NOT NULL AND NEW.close_date IS NOT NULL 
       AND NEW.open_date > NEW.close_date THEN
        RAISE EXCEPTION 'ê°œì—…ì¼ì´ íì—…ì¼ë³´ë‹¤ ëŠ¦ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
    END IF;
    
    -- 4. ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ìë™ ê³„ì‚°
    NEW.data_quality_score = calculate_business_quality_score(NEW);
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER business_validation_trigger
    BEFORE INSERT OR UPDATE ON businesses
    FOR EACH ROW
    EXECUTE FUNCTION validate_business_data();

-- í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
CREATE OR REPLACE FUNCTION calculate_business_quality_score(business businesses)
RETURNS INTEGER AS $$
DECLARE
    score INTEGER := 0;
BEGIN
    -- í•„ìˆ˜ ì •ë³´ ì™„ì„±ë„ (40ì )
    IF business.name IS NOT NULL AND LENGTH(TRIM(business.name)) >= 2 THEN
        score := score + 20;
    END IF;
    
    IF business.hotplace_id IS NOT NULL THEN
        score := score + 10;
    END IF;
    
    IF business.category_id IS NOT NULL THEN
        score := score + 10;
    END IF;
    
    -- ì¶”ê°€ ì •ë³´ ì™„ì„±ë„ (30ì )
    IF business.map_link IS NOT NULL AND business.map_link != '' THEN
        score := score + 15;
    END IF;
    
    IF business.description IS NOT NULL AND LENGTH(business.description) > 10 THEN
        score := score + 10;
    END IF;
    
    IF jsonb_array_length(business.keywords) > 0 THEN
        score := score + 5;
    END IF;
    
    -- ì£¼ì†Œ ì •ë³´ ì™„ì„±ë„ (20ì )
    IF business.address_detail IS NOT NULL AND LENGTH(business.address_detail) > 10 THEN
        score := score + 20;
    END IF;
    
    -- ìš´ì˜ ì •ë³´ ì™„ì„±ë„ (10ì )
    IF business.open_date IS NOT NULL THEN
        score := score + 10;
    END IF;
    
    RETURN score;
END;
$$ LANGUAGE plpgsql;
```

---

## 5. ë°±ì—… ë° ë³´ì•ˆ ì „ëµ

### 5.1 ë°±ì—… ì „ëµ

**ğŸ¯ íŒ€ì¥í‚´ì˜ ë°±ì—… ê´€ë¦¬ ë°©ì¹¨**:
1. **3-2-1 ì›ì¹™**: 3ê°œ ë³µì‚¬ë³¸, 2ê°œ ë‹¤ë¥¸ ë¯¸ë””ì–´, 1ê°œ ì›ê²© ì €ì¥
2. **ìë™í™”**: ìˆ˜ë™ ê°œì… ìµœì†Œí™”
3. **ë³µêµ¬ í…ŒìŠ¤íŠ¸**: ì •ê¸°ì  ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
4. **ë¬¸ì„œí™”**: ëª¨ë“  ë°±ì—… ì ˆì°¨ ë¬¸ì„œí™”

```bash
#!/bin/bash
# backup_manager.sh

# ë°±ì—… ì„¤ì •
DB_NAME="ocler_db"
DB_USER="ocler_user" 
BACKUP_DIR="/backup/ocler"
REMOTE_BACKUP_DIR="s3://ocler-backup-bucket"
RETENTION_DAYS=30

# ë¡œê·¸ ì„¤ì •
LOG_FILE="/var/log/ocler_backup.log"
DATE=$(date '+%Y%m%d_%H%M%S')

log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… í•¨ìˆ˜
backup_database() {
    local backup_file="${BACKUP_DIR}/db_backup_${DATE}.sql"
    local compressed_file="${backup_file}.gz"
    
    log_message "ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹œì‘: $DB_NAME"
    
    # PostgreSQL ë¤í”„ ìƒì„±
    pg_dump -h localhost -U $DB_USER -d $DB_NAME \
            --verbose --clean --no-owner --no-privileges \
            --file=$backup_file
    
    if [ $? -eq 0 ]; then
        # ì••ì¶•
        gzip $backup_file
        log_message "ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: $compressed_file"
        
        # ì›ê²© ì €ì¥ì†Œì— ì—…ë¡œë“œ
        aws s3 cp $compressed_file $REMOTE_BACKUP_DIR/
        if [ $? -eq 0 ]; then
            log_message "ì›ê²© ë°±ì—… ì—…ë¡œë“œ ì™„ë£Œ"
        else
            log_message "ERROR: ì›ê²© ë°±ì—… ì—…ë¡œë“œ ì‹¤íŒ¨"
        fi
    else
        log_message "ERROR: ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤íŒ¨"
        exit 1
    fi
}

# CSV ì›ë³¸ ë°ì´í„° ë°±ì—…
backup_csv_data() {
    local csv_backup_dir="${BACKUP_DIR}/csv_${DATE}"
    mkdir -p $csv_backup_dir
    
    log_message "CSV ë°ì´í„° ë°±ì—… ì‹œì‘"
    
    # CSV íŒŒì¼ë“¤ ë³µì‚¬
    cp /data/csv/*.csv $csv_backup_dir/
    
    # ì••ì¶• ì•„ì¹´ì´ë¸Œ ìƒì„±
    tar -czf "${csv_backup_dir}.tar.gz" -C $BACKUP_DIR "csv_${DATE}"
    rm -rf $csv_backup_dir
    
    # ì›ê²© ì €ì¥ì†Œ ì—…ë¡œë“œ
    aws s3 cp "${csv_backup_dir}.tar.gz" $REMOTE_BACKUP_DIR/
    
    log_message "CSV ë°ì´í„° ë°±ì—… ì™„ë£Œ"
}

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë°±ì—…
backup_app_config() {
    local config_backup_file="${BACKUP_DIR}/app_config_${DATE}.tar.gz"
    
    log_message "ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë°±ì—… ì‹œì‘"
    
    tar -czf $config_backup_file \
        /app/config/ \
        /app/.env \
        /etc/nginx/sites-available/ocler \
        /etc/systemd/system/ocler.service
    
    aws s3 cp $config_backup_file $REMOTE_BACKUP_DIR/
    
    log_message "ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë°±ì—… ì™„ë£Œ"
}

# ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
cleanup_old_backups() {
    log_message "ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬ ì‹œì‘ (${RETENTION_DAYS}ì¼ ì´ìƒ)"
    
    # ë¡œì»¬ ë°±ì—… ì •ë¦¬
    find $BACKUP_DIR -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete
    find $BACKUP_DIR -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete
    
    # S3 ë°±ì—… ì •ë¦¬ (lifecycle policyë¡œ ëŒ€ì²´ ê°€ëŠ¥)
    aws s3 ls $REMOTE_BACKUP_DIR/ | while read -r line; do
        file_date=$(echo $line | awk '{print $1}')
        file_name=$(echo $line | awk '{print $4}')
        
        if [[ $(date -d "$file_date" +%s) -lt $(date -d "$RETENTION_DAYS days ago" +%s) ]]; then
            aws s3 rm "${REMOTE_BACKUP_DIR}/${file_name}"
            log_message "ì˜¤ë˜ëœ ì›ê²© ë°±ì—… ì‚­ì œ: $file_name"
        fi
    done
    
    log_message "ë°±ì—… ì •ë¦¬ ì™„ë£Œ"
}

# ë°±ì—… ê²€ì¦
verify_backup() {
    local backup_file="${BACKUP_DIR}/db_backup_${DATE}.sql.gz"
    
    if [ -f "$backup_file" ]; then
        # ì••ì¶• íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
        if gzip -t "$backup_file"; then
            log_message "ë°±ì—… íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦ ì„±ê³µ"
            
            # ë°±ì—… íŒŒì¼ í¬ê¸° í™•ì¸ (ìµœì†Œ í¬ê¸° ì²´í¬)
            file_size=$(stat -c%s "$backup_file")
            min_size=1048576  # 1MB
            
            if [ $file_size -gt $min_size ]; then
                log_message "ë°±ì—… íŒŒì¼ í¬ê¸° ê²€ì¦ ì„±ê³µ: ${file_size} bytes"
            else
                log_message "WARNING: ë°±ì—… íŒŒì¼ í¬ê¸°ê°€ ì˜ˆìƒë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤: ${file_size} bytes"
            fi
        else
            log_message "ERROR: ë°±ì—… íŒŒì¼ ì†ìƒ ê°ì§€"
            exit 1
        fi
    else
        log_message "ERROR: ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        exit 1
    fi
}

# ë©”ì¸ ì‹¤í–‰
main() {
    log_message "=== ì˜¤í´ëŸ¬ ë°±ì—… í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ==="
    
    # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p $BACKUP_DIR
    
    # ë°±ì—… ì‹¤í–‰
    backup_database
    backup_csv_data
    backup_app_config
    
    # ë°±ì—… ê²€ì¦
    verify_backup
    
    # ì •ë¦¬ ì‘ì—…
    cleanup_old_backups
    
    # ë°±ì—… ì™„ë£Œ ì•Œë¦¼
    log_message "=== ë°±ì—… í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ==="
    
    # Slack ì•Œë¦¼ (ì„ íƒì‚¬í•­)
    if [ ! -z "$SLACK_WEBHOOK_URL" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"âœ… ì˜¤í´ëŸ¬ DB ë°±ì—… ì™„ë£Œ: $(date)\"}" \
            $SLACK_WEBHOOK_URL
    fi
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
```

### 5.2 ë³´ì•ˆ ì „ëµ

```python
# security_manager.py
import hashlib
import secrets
import jwt
from datetime import datetime, timedelta
from cryptography.fernet import Fernet
import bcrypt

class SecurityManager:
    def __init__(self, secret_key: str):
        self.secret_key = secret_key
        self.fernet = Fernet(Fernet.generate_key())
        
    def hash_password(self, password: str) -> str:
        """ë¹„ë°€ë²ˆí˜¸ í•´ì‹±"""
        salt = bcrypt.gensalt()
        hashed = bcrypt.hashpw(password.encode('utf-8'), salt)
        return hashed.decode('utf-8')
    
    def verify_password(self, password: str, hashed: str) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
        return bcrypt.checkpw(password.encode('utf-8'), hashed.encode('utf-8'))
    
    def generate_api_key(self, user_id: str) -> str:
        """API í‚¤ ìƒì„±"""
        timestamp = str(int(datetime.now().timestamp()))
        raw_key = f"{user_id}:{timestamp}:{secrets.token_hex(16)}"
        return hashlib.sha256(raw_key.encode()).hexdigest()
    
    def encrypt_sensitive_data(self, data: str) -> str:
        """ë¯¼ê°í•œ ë°ì´í„° ì•”í˜¸í™”"""
        return self.fernet.encrypt(data.encode()).decode()
    
    def decrypt_sensitive_data(self, encrypted_data: str) -> str:
        """ì•”í˜¸í™”ëœ ë°ì´í„° ë³µí˜¸í™”"""
        return self.fernet.decrypt(encrypted_data.encode()).decode()
    
    def generate_jwt_token(self, user_id: str, role: str) -> str:
        """JWT í† í° ìƒì„±"""
        payload = {
            'user_id': user_id,
            'role': role,
            'exp': datetime.utcnow() + timedelta(hours=24),
            'iat': datetime.utcnow()
        }
        return jwt.encode(payload, self.secret_key, algorithm='HS256')
    
    def verify_jwt_token(self, token: str) -> dict:
        """JWT í† í° ê²€ì¦"""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=['HS256'])
            return payload
        except jwt.ExpiredSignatureError:
            raise Exception("í† í°ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
        except jwt.InvalidTokenError:
            raise Exception("ìœ íš¨í•˜ì§€ ì•Šì€ í† í°ì…ë‹ˆë‹¤")

# ë°ì´í„°ë² ì´ìŠ¤ ë³´ì•ˆ ì„¤ì •
class DatabaseSecurity:
    @staticmethod
    def setup_row_level_security():
        """í–‰ ìˆ˜ì¤€ ë³´ì•ˆ ì„¤ì •"""
        return """
        -- ì‚¬ìš©ìë³„ ë°ì´í„° ì ‘ê·¼ ì œí•œ
        ALTER TABLE businesses ENABLE ROW LEVEL SECURITY;
        
        -- ì¼ë°˜ ì‚¬ìš©ìëŠ” í™œì„± ì—…ì²´ë§Œ ì¡°íšŒ ê°€ëŠ¥
        CREATE POLICY "public_businesses_select" ON businesses
        FOR SELECT TO authenticated
        USING (status = 'active');
        
        -- ì—…ì²´ ì†Œìœ ìëŠ” ìì‹ ì˜ ì—…ì²´ ì •ë³´ ìˆ˜ì • ê°€ëŠ¥
        CREATE POLICY "owner_businesses_update" ON businesses
        FOR UPDATE TO authenticated
        USING (
            EXISTS (
                SELECT 1 FROM user_profiles 
                WHERE id = auth.uid() 
                AND owned_business_id = businesses.id
            )
        );
        
        -- ê´€ë¦¬ìëŠ” ëª¨ë“  ë°ì´í„° ì ‘ê·¼ ê°€ëŠ¥
        CREATE POLICY "admin_businesses_all" ON businesses
        FOR ALL TO authenticated
        USING (
            EXISTS (
                SELECT 1 FROM user_profiles 
                WHERE id = auth.uid() 
                AND role = 'admin'
            )
        );
        """
    
    @staticmethod
    def setup_audit_logging():
        """ê°ì‚¬ ë¡œê·¸ ì„¤ì •"""
        return """
        -- ê°ì‚¬ ë¡œê·¸ í…Œì´ë¸”
        CREATE TABLE audit_logs (
            id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
            table_name VARCHAR(50) NOT NULL,
            record_id UUID,
            action VARCHAR(10) NOT NULL, -- INSERT, UPDATE, DELETE
            old_values JSONB,
            new_values JSONB,
            user_id UUID,
            ip_address INET,
            user_agent TEXT,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );
        
        -- ê°ì‚¬ ë¡œê·¸ íŠ¸ë¦¬ê±° í•¨ìˆ˜
        CREATE OR REPLACE FUNCTION audit_trigger_function()
        RETURNS TRIGGER AS $$
        BEGIN
            INSERT INTO audit_logs (
                table_name, record_id, action, old_values, new_values, 
                user_id, ip_address
            ) VALUES (
                TG_TABLE_NAME,
                COALESCE(NEW.id, OLD.id),
                TG_OP,
                CASE WHEN TG_OP = 'DELETE' THEN row_to_json(OLD) ELSE NULL END,
                CASE WHEN TG_OP IN ('INSERT', 'UPDATE') THEN row_to_json(NEW) ELSE NULL END,
                auth.uid(),
                inet_client_addr()
            );
            
            RETURN COALESCE(NEW, OLD);
        END;
        $$ LANGUAGE plpgsql;
        
        -- ì—…ì²´ í…Œì´ë¸”ì— ê°ì‚¬ íŠ¸ë¦¬ê±° ì ìš©
        CREATE TRIGGER businesses_audit_trigger
            AFTER INSERT OR UPDATE OR DELETE ON businesses
            FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();
        """
```

---

## 6. ë°ì´í„° ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤

### 6.1 ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ

**ğŸ¯ íŒ€ì¥í‚´ì˜ ì—…ë°ì´íŠ¸ ê´€ë¦¬ ì „ëµ**:
1. **ë²„ì €ë‹**: ëª¨ë“  ë°ì´í„° ë³€ê²½ ì´ë ¥ ì¶”ì 
2. **ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤**: ì¤‘ìš” ë³€ê²½ì‚¬í•­ ê²€í†  ë‹¨ê³„
3. **ë¡¤ë°± ì‹œìŠ¤í…œ**: ë¬¸ì œ ë°œìƒì‹œ ì¦‰ì‹œ ë³µêµ¬
4. **ì•Œë¦¼ ì‹œìŠ¤í…œ**: ì´í•´ê´€ê³„ì ìë™ ì•Œë¦¼

```python
# update_manager.py
from enum import Enum
from typing import Dict, List, Optional
import json
from datetime import datetime

class UpdateType(Enum):
    CREATE = "create"
    UPDATE = "update" 
    DELETE = "delete"
    BULK_UPDATE = "bulk_update"

class UpdateStatus(Enum):
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    APPLIED = "applied"
    FAILED = "failed"

class DataUpdateManager:
    def __init__(self, db_connection, notification_service):
        self.db = db_connection
        self.notifications = notification_service
        
    def create_update_request(self, 
                            requester_id: str,
                            update_type: UpdateType,
                            table_name: str,
                            record_id: Optional[str],
                            changes: Dict,
                            reason: str) -> str:
        """ì—…ë°ì´íŠ¸ ìš”ì²­ ìƒì„±"""
        
        cursor = self.db.cursor()
        
        # ì—…ë°ì´íŠ¸ ìš”ì²­ ë ˆì½”ë“œ ìƒì„±
        request_data = {
            'id': str(uuid.uuid4()),
            'requester_id': requester_id,
            'update_type': update_type.value,
            'table_name': table_name,
            'record_id': record_id,
            'changes': json.dumps(changes, ensure_ascii=False),
            'reason': reason,
            'status': UpdateStatus.PENDING.value,
            'created_at': datetime.now()
        }
        
        cursor.execute("""
            INSERT INTO update_requests (
                id, requester_id, update_type, table_name, record_id,
                changes, reason, status, created_at
            ) VALUES (
                %(id)s, %(requester_id)s, %(update_type)s, %(table_name)s, 
                %(record_id)s, %(changes)s, %(reason)s, %(status)s, %(created_at)s
            )
        """, request_data)
        
        self.db.commit()
        
        # ìŠ¹ì¸ìì—ê²Œ ì•Œë¦¼ ë°œì†¡
        self._notify_approvers(request_data['id'], update_type, table_name)
        
        return request_data['id']
    
    def approve_update_request(self, request_id: str, approver_id: str) -> bool:
        """ì—…ë°ì´íŠ¸ ìš”ì²­ ìŠ¹ì¸"""
        cursor = self.db.cursor()
        
        # ìš”ì²­ ì •ë³´ ì¡°íšŒ
        cursor.execute("""
            SELECT * FROM update_requests 
            WHERE id = %s AND status = %s
        """, (request_id, UpdateStatus.PENDING.value))
        
        request = cursor.fetchone()
        if not request:
            return False
        
        try:
            # ë³€ê²½ì‚¬í•­ ì ìš©
            self._apply_changes(request)
            
            # ìš”ì²­ ìƒíƒœ ì—…ë°ì´íŠ¸
            cursor.execute("""
                UPDATE update_requests 
                SET status = %s, approver_id = %s, approved_at = NOW()
                WHERE id = %s
            """, (UpdateStatus.APPLIED.value, approver_id, request_id))
            
            self.db.commit()
            
            # ìš”ì²­ìì—ê²Œ ìŠ¹ì¸ ì•Œë¦¼
            self._notify_requester(request['requester_id'], request_id, 'approved')
            
            return True
            
        except Exception as e:
            # ì‹¤íŒ¨ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
            cursor.execute("""
                UPDATE update_requests 
                SET status = %s, error_message = %s
                WHERE id = %s
            """, (UpdateStatus.FAILED.value, str(e), request_id))
            
            self.db.rollback()
            return False
    
    def _apply_changes(self, request: Dict):
        """ë³€ê²½ì‚¬í•­ ì‹¤ì œ ì ìš©"""
        cursor = self.db.cursor()
        changes = json.loads(request['changes'])
        
        if request['update_type'] == UpdateType.CREATE.value:
            # ìƒˆ ë ˆì½”ë“œ ìƒì„±
            self._create_record(cursor, request['table_name'], changes)
            
        elif request['update_type'] == UpdateType.UPDATE.value:
            # ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
            self._update_record(cursor, request['table_name'], 
                              request['record_id'], changes)
            
        elif request['update_type'] == UpdateType.DELETE.value:
            # ë ˆì½”ë“œ ì‚­ì œ (ì†Œí”„íŠ¸ ì‚­ì œ)
            cursor.execute(f"""
                UPDATE {request['table_name']} 
                SET status = 'deleted', updated_at = NOW()
                WHERE id = %s
            """, (request['record_id'],))
            
        elif request['update_type'] == UpdateType.BULK_UPDATE.value:
            # ëŒ€ëŸ‰ ì—…ë°ì´íŠ¸
            self._bulk_update_records(cursor, request['table_name'], changes)
    
    def _create_record(self, cursor, table_name: str, data: Dict):
        """ìƒˆ ë ˆì½”ë“œ ìƒì„±"""
        columns = ', '.join(data.keys())
        placeholders = ', '.join(['%s'] * len(data))
        values = list(data.values())
        
        cursor.execute(f"""
            INSERT INTO {table_name} ({columns})
            VALUES ({placeholders})
        """, values)
    
    def _update_record(self, cursor, table_name: str, record_id: str, changes: Dict):
        """ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸"""
        set_clause = ', '.join([f"{key} = %s" for key in changes.keys()])
        values = list(changes.values()) + [record_id]
        
        cursor.execute(f"""
            UPDATE {table_name} 
            SET {set_clause}, updated_at = NOW()
            WHERE id = %s
        """, values)
    
    def _bulk_update_records(self, cursor, table_name: str, bulk_changes: Dict):
        """ëŒ€ëŸ‰ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸"""
        # ì¡°ê±´ê³¼ ë³€ê²½ì‚¬í•­ì— ë”°ë¼ êµ¬í˜„
        conditions = bulk_changes.get('conditions', {})
        updates = bulk_changes.get('updates', {})
        
        where_clause = ' AND '.join([f"{key} = %s" for key in conditions.keys()])
        set_clause = ', '.join([f"{key} = %s" for key in updates.keys()])
        
        values = list(updates.values()) + list(conditions.values())
        
        cursor.execute(f"""
            UPDATE {table_name}
            SET {set_clause}, updated_at = NOW()
            WHERE {where_clause}
        """, values)
    
    def get_update_history(self, table_name: str, record_id: str) -> List[Dict]:
        """ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ ì´ë ¥ ì¡°íšŒ"""
        cursor = self.db.cursor()
        
        cursor.execute("""
            SELECT ur.*, up.name as requester_name, ap.name as approver_name
            FROM update_requests ur
            LEFT JOIN user_profiles up ON ur.requester_id = up.id
            LEFT JOIN user_profiles ap ON ur.approver_id = ap.id
            WHERE ur.table_name = %s AND ur.record_id = %s
            ORDER BY ur.created_at DESC
        """, (table_name, record_id))
        
        return [dict(row) for row in cursor.fetchall()]
    
    def _notify_approvers(self, request_id: str, update_type: UpdateType, table_name: str):
        """ìŠ¹ì¸ìì—ê²Œ ì•Œë¦¼ ë°œì†¡"""
        message = f"ìƒˆë¡œìš´ ë°ì´í„° ì—…ë°ì´íŠ¸ ìš”ì²­: {table_name} {update_type.value}"
        self.notifications.send_to_admins(message, {'request_id': request_id})
    
    def _notify_requester(self, requester_id: str, request_id: str, status: str):
        """ìš”ì²­ìì—ê²Œ ì•Œë¦¼ ë°œì†¡"""
        message = f"ë°ì´í„° ì—…ë°ì´íŠ¸ ìš”ì²­ì´ {status}ë˜ì—ˆìŠµë‹ˆë‹¤."
        self.notifications.send_to_user(requester_id, message, {'request_id': request_id})

# ìë™ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬
class AutoUpdateScheduler:
    def __init__(self, db_connection):
        self.db = db_connection
        
    def schedule_csv_sync(self):
        """CSV íŒŒì¼ê³¼ DB ë™ê¸°í™” ìŠ¤ì¼€ì¤„"""
        # ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ì‹¤í–‰
        import schedule
        
        schedule.every().day.at("03:00").do(self.sync_csv_to_db)
        
    def sync_csv_to_db(self):
        """CSV íŒŒì¼ì˜ ë³€ê²½ì‚¬í•­ì„ DBì— ë°˜ì˜"""
        try:
            # 1. CSV íŒŒì¼ ë¡œë“œ
            new_data = pd.read_csv('/data/updated_businesses.csv')
            
            # 2. ë³€ê²½ì‚¬í•­ ê°ì§€
            changes = self._detect_changes(new_data)
            
            # 3. ë³€ê²½ì‚¬í•­ ì ìš©
            for change in changes:
                self._apply_auto_update(change)
                
            logger.info(f"ìë™ ë™ê¸°í™” ì™„ë£Œ: {len(changes)}ê°œ ë³€ê²½ì‚¬í•­ ì ìš©")
            
        except Exception as e:
            logger.error(f"ìë™ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _detect_changes(self, new_data: pd.DataFrame) -> List[Dict]:
        """DBì™€ CSV ë°ì´í„° ë¹„êµí•˜ì—¬ ë³€ê²½ì‚¬í•­ ê°ì§€"""
        cursor = self.db.cursor()
        
        # í˜„ì¬ DB ë°ì´í„° ì¡°íšŒ
        cursor.execute("SELECT * FROM businesses")
        current_data = pd.DataFrame(cursor.fetchall())
        
        changes = []
        
        # ì‹ ê·œ ì—…ì²´ ê°ì§€
        new_ids = set(new_data['id']) - set(current_data['original_id'])
        for new_id in new_ids:
            new_record = new_data[new_data['id'] == new_id].iloc[0]
            changes.append({
                'type': 'create',
                'data': new_record.to_dict()
            })
        
        # ì—…ë°ì´íŠ¸ëœ ì—…ì²´ ê°ì§€
        common_ids = set(new_data['id']) & set(current_data['original_id'])
        for common_id in common_ids:
            new_record = new_data[new_data['id'] == common_id].iloc[0]
            current_record = current_data[current_data['original_id'] == common_id].iloc[0]
            
            # í•„ë“œë³„ ë¹„êµ
            changed_fields = {}
            for field in new_record.index:
                if field in current_record.index:
                    if new_record[field] != current_record[field]:
                        changed_fields[field] = new_record[field]
            
            if changed_fields:
                changes.append({
                    'type': 'update',
                    'id': common_id,
                    'changes': changed_fields
                })
        
        return changes
```

### 6.2 ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ

```sql
-- ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‘ì—… í…Œì´ë¸”
CREATE TABLE batch_update_jobs (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    job_name VARCHAR(100) NOT NULL,
    job_type VARCHAR(50) NOT NULL, -- 'csv_import', 'data_cleanup', 'migration'
    parameters JSONB DEFAULT '{}'::jsonb,
    status VARCHAR(20) DEFAULT 'pending', -- pending, running, completed, failed
    total_records INTEGER DEFAULT 0,
    processed_records INTEGER DEFAULT 0,
    error_records INTEGER DEFAULT 0,
    start_time TIMESTAMP WITH TIME ZONE,
    end_time TIMESTAMP WITH TIME ZONE,
    error_log TEXT,
    created_by UUID REFERENCES user_profiles(id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION process_batch_update_job(job_id UUID)
RETURNS VOID AS $$
DECLARE
    job_record batch_update_jobs%ROWTYPE;
    processed_count INTEGER := 0;
    error_count INTEGER := 0;
BEGIN
    -- ì‘ì—… ì •ë³´ ì¡°íšŒ
    SELECT * INTO job_record FROM batch_update_jobs WHERE id = job_id;
    
    IF NOT FOUND THEN
        RAISE EXCEPTION 'ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: %', job_id;
    END IF;
    
    -- ì‘ì—… ì‹œì‘
    UPDATE batch_update_jobs 
    SET status = 'running', start_time = NOW()
    WHERE id = job_id;
    
    -- ì‘ì—… íƒ€ì…ë³„ ì²˜ë¦¬
    CASE job_record.job_type
        WHEN 'csv_import' THEN
            -- CSV ì„í¬íŠ¸ ë¡œì§
            PERFORM import_csv_data(job_record.parameters);
            
        WHEN 'data_cleanup' THEN
            -- ë°ì´í„° ì •ë¦¬ ë¡œì§
            PERFORM cleanup_duplicate_businesses();
            
        WHEN 'status_update' THEN
            -- ì—…ì²´ ìƒíƒœ ì¼ê´„ ì—…ë°ì´íŠ¸
            PERFORM update_business_status_batch(job_record.parameters);
            
        ELSE
            RAISE EXCEPTION 'ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—… íƒ€ì…: %', job_record.job_type;
    END CASE;
    
    -- ì‘ì—… ì™„ë£Œ
    UPDATE batch_update_jobs 
    SET 
        status = 'completed',
        end_time = NOW(),
        processed_records = processed_count
    WHERE id = job_id;
    
EXCEPTION
    WHEN OTHERS THEN
        -- ì˜¤ë¥˜ ë°œìƒì‹œ ë¡œê·¸ ê¸°ë¡
        UPDATE batch_update_jobs 
        SET 
            status = 'failed',
            end_time = NOW(),
            error_log = SQLERRM
        WHERE id = job_id;
        
        RAISE;
END;
$$ LANGUAGE plpgsql;

-- ì¤‘ë³µ ì—…ì²´ ì •ë¦¬ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION cleanup_duplicate_businesses()
RETURNS INTEGER AS $$
DECLARE
    duplicate_count INTEGER := 0;
    duplicate_record RECORD;
BEGIN
    -- ì´ë¦„ê³¼ ìœ„ì¹˜ê°€ ë™ì¼í•œ ì¤‘ë³µ ì—…ì²´ ì°¾ê¸°
    FOR duplicate_record IN
        SELECT name, hotplace_id, array_agg(id) as duplicate_ids
        FROM businesses
        WHERE status != 'deleted'
        GROUP BY name, hotplace_id
        HAVING COUNT(*) > 1
    LOOP
        -- ê°€ì¥ ìµœê·¼ ë ˆì½”ë“œë¥¼ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì¤‘ë³µìœ¼ë¡œ ë§ˆí‚¹
        UPDATE businesses 
        SET 
            status = 'duplicate',
            updated_at = NOW()
        WHERE id = ANY(duplicate_record.duplicate_ids[2:]);
        
        duplicate_count := duplicate_count + array_length(duplicate_record.duplicate_ids, 1) - 1;
    END LOOP;
    
    RETURN duplicate_count;
END;
$$ LANGUAGE plpgsql;
```

---

## 7. ê°œë°œíŒ€ì´ ì‚¬ìš©í•  ë°ì´í„° ì ‘ê·¼ ë°©ë²•

### 7.1 RESTful API ì„¤ê³„

**ğŸ¯ íŒ€ì¥í‚´ì˜ API ì„¤ê³„ ì›ì¹™**:
1. **ì§ê´€ì  ì—”ë“œí¬ì¸íŠ¸**: ë¦¬ì†ŒìŠ¤ ì¤‘ì‹¬ URL êµ¬ì¡°
2. **ì¼ê´€ëœ ì‘ë‹µ í˜•ì‹**: í‘œì¤€í™”ëœ JSON ì‘ë‹µ
3. **ì—ëŸ¬ ì²˜ë¦¬**: ëª…í™•í•œ ì—ëŸ¬ ì½”ë“œì™€ ë©”ì‹œì§€
4. **ì„±ëŠ¥ ìµœì í™”**: í˜ì´ì§€ë„¤ì´ì…˜, ìºì‹±, ì••ì¶•

```python
# api_endpoints.py
from flask import Flask, request, jsonify
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
import redis
import json
from typing import Dict, List, Optional

app = Flask(__name__)

# Rate limiting ì„¤ì •
limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["1000 per hour"]
)

# Redis ìºì‹œ ì„¤ì •
redis_client = redis.Redis(host='localhost', port=6379, db=0)

class BusinessAPI:
    def __init__(self, db_connection):
        self.db = db_connection
        
    @app.route('/api/v1/businesses', methods=['GET'])
    @limiter.limit("100 per minute")
    def get_businesses(self):
        """ì—…ì²´ ëª©ë¡ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜, í•„í„°ë§, ê²€ìƒ‰ ì§€ì›)"""
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° íŒŒì‹±
        page = request.args.get('page', 1, type=int)
        per_page = min(request.args.get('per_page', 20, type=int), 100)
        hotplace = request.args.get('hotplace')
        category = request.args.get('category')
        status = request.args.get('status', 'active')
        search = request.args.get('search')
        sort = request.args.get('sort', 'name')
        order = request.args.get('order', 'asc')
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"businesses:{page}:{per_page}:{hotplace}:{category}:{status}:{search}:{sort}:{order}"
        
        # ìºì‹œ í™•ì¸
        cached_result = redis_client.get(cache_key)
        if cached_result:
            return json.loads(cached_result)
        
        try:
            cursor = self.db.cursor()
            
            # ê¸°ë³¸ ì¿¼ë¦¬
            base_query = """
                SELECT 
                    b.id, b.original_id, b.name, b.address_detail,
                    b.status, b.view_count, b.favorite_count,
                    h.name as hotplace_name, h.display_name as hotplace_display,
                    c.name as category_name, c.display_name as category_display,
                    sc.name as subcategory_name,
                    b.keywords, b.thumbnail, b.map_link,
                    b.has_parking, b.has_event,
                    b.data_quality_score,
                    b.created_at, b.updated_at
                FROM business_details b
                LEFT JOIN hotplaces h ON b.hotplace_id = h.id
                LEFT JOIN categories c ON b.category_id = c.id
                LEFT JOIN subcategories sc ON b.subcategory_id = sc.id
                WHERE 1=1
            """
            
            params = []
            
            # í•„í„° ì¡°ê±´ ì¶”ê°€
            if status:
                base_query += " AND b.status = %s"
                params.append(status)
            
            if hotplace:
                base_query += " AND h.name = %s"
                params.append(hotplace)
            
            if category:
                base_query += " AND c.name = %s"
                params.append(category)
            
            if search:
                base_query += " AND (b.search_vector @@ plainto_tsquery('korean', %s) OR b.name ILIKE %s)"
                params.extend([search, f'%{search}%'])
            
            # ì •ë ¬ ì¡°ê±´ ì¶”ê°€
            valid_sort_fields = ['name', 'created_at', 'view_count', 'favorite_count', 'data_quality_score']
            if sort in valid_sort_fields:
                order_clause = f" ORDER BY b.{sort} {'ASC' if order.lower() == 'asc' else 'DESC'}"
                base_query += order_clause
            
            # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
            count_query = f"SELECT COUNT(*) FROM ({base_query}) as counted"
            cursor.execute(count_query, params)
            total_count = cursor.fetchone()[0]
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
            offset = (page - 1) * per_page
            paginated_query = f"{base_query} LIMIT %s OFFSET %s"
            params.extend([per_page, offset])
            
            cursor.execute(paginated_query, params)
            businesses = [dict(row) for row in cursor.fetchall()]
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
            response_data = {
                'status': 'success',
                'data': {
                    'businesses': businesses,
                    'pagination': {
                        'page': page,
                        'per_page': per_page,
                        'total': total_count,
                        'pages': (total_count + per_page - 1) // per_page
                    },
                    'filters': {
                        'hotplace': hotplace,
                        'category': category,
                        'status': status,
                        'search': search
                    }
                },
                'meta': {
                    'generated_at': datetime.utcnow().isoformat(),
                    'cache_ttl': 300  # 5ë¶„
                }
            }
            
            # ìºì‹œ ì €ì¥ (5ë¶„ TTL)
            redis_client.setex(cache_key, 300, json.dumps(response_data, default=str, ensure_ascii=False))
            
            return jsonify(response_data)
            
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': 'ì—…ì²´ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                'error': str(e)
            }), 500
    
    @app.route('/api/v1/businesses/<business_id>', methods=['GET'])
    @limiter.limit("200 per minute")
    def get_business_detail(self, business_id: str):
        """ì—…ì²´ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        
        cache_key = f"business_detail:{business_id}"
        cached_result = redis_client.get(cache_key)
        if cached_result:
            # ì¡°íšŒìˆ˜ ì¦ê°€
            self._increment_view_count(business_id)
            return json.loads(cached_result)
        
        try:
            cursor = self.db.cursor()
            
            # ì—…ì²´ ìƒì„¸ ì •ë³´ ì¡°íšŒ
            cursor.execute("""
                SELECT 
                    b.*,
                    h.name as hotplace_name, h.display_name as hotplace_display,
                    h.emoji as hotplace_emoji, h.district as hotplace_district,
                    c.name as category_name, c.display_name as category_display,
                    c.emoji as category_emoji,
                    sc.name as subcategory_name, sc.display_name as subcategory_display,
                    -- ìš´ì˜ ìƒíƒœ ê³„ì‚°
                    CASE 
                        WHEN b.status = 'closed' OR b.close_date IS NOT NULL THEN 'closed'
                        WHEN b.status = 'temporary_closed' THEN 'temporary_closed'
                        ELSE 'active'
                    END as computed_status,
                    -- ì‹ ê·œ ì—…ì²´ ì—¬ë¶€
                    (b.open_date >= CURRENT_DATE - INTERVAL '3 months') as is_new_business,
                    -- ê´€ë ¨ í†µê³„
                    (SELECT COUNT(*) FROM favorites WHERE business_id = b.id) as favorite_count_real
                FROM businesses b
                LEFT JOIN hotplaces h ON b.hotplace_id = h.id
                LEFT JOIN categories c ON b.category_id = c.id
                LEFT JOIN subcategories sc ON b.subcategory_id = sc.id
                WHERE b.id = %s OR b.original_id = %s
            """, (business_id, business_id))
            
            business = cursor.fetchone()
            
            if not business:
                return jsonify({
                    'status': 'error',
                    'message': 'ì—…ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }), 404
            
            business_dict = dict(business)
            
            # ì¡°íšŒìˆ˜ ì¦ê°€
            self._increment_view_count(business_dict['id'])
            business_dict['view_count'] += 1
            
            # ê´€ë ¨ ì—…ì²´ ì¶”ì²œ (ê°™ì€ ì§€ì—­, ê°™ì€ ì¹´í…Œê³ ë¦¬)
            cursor.execute("""
                SELECT id, name, thumbnail, view_count
                FROM businesses
                WHERE hotplace_id = %s AND category_id = %s 
                AND id != %s AND status = 'active'
                ORDER BY view_count DESC
                LIMIT 5
            """, (business_dict['hotplace_id'], business_dict['category_id'], business_dict['id']))
            
            related_businesses = [dict(row) for row in cursor.fetchall()]
            
            response_data = {
                'status': 'success',
                'data': {
                    'business': business_dict,
                    'related_businesses': related_businesses
                },
                'meta': {
                    'generated_at': datetime.utcnow().isoformat()
                }
            }
            
            # ìºì‹œ ì €ì¥ (10ë¶„ TTL)
            redis_client.setex(cache_key, 600, json.dumps(response_data, default=str, ensure_ascii=False))
            
            return jsonify(response_data)
            
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': 'ì—…ì²´ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                'error': str(e)
            }), 500
    
    @app.route('/api/v1/search', methods=['GET'])
    @limiter.limit("50 per minute")
    def search_businesses(self):
        """ê³ ê¸‰ ê²€ìƒ‰ API (ì „ë¬¸ê²€ìƒ‰ ì§€ì›)"""
        
        query = request.args.get('q', '').strip()
        if not query:
            return jsonify({
                'status': 'error',
                'message': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'
            }), 400
        
        page = request.args.get('page', 1, type=int)
        per_page = min(request.args.get('per_page', 20, type=int), 50)
        
        try:
            cursor = self.db.cursor()
            
            # ì „ë¬¸ê²€ìƒ‰ ì¿¼ë¦¬ (PostgreSQL Full Text Search)
            search_query = """
                SELECT 
                    b.id, b.name, b.address_detail, b.thumbnail,
                    h.name as hotplace_name, c.name as category_name,
                    b.view_count, b.favorite_count,
                    ts_rank(b.search_vector, plainto_tsquery('korean', %s)) as rank
                FROM businesses b
                LEFT JOIN hotplaces h ON b.hotplace_id = h.id
                LEFT JOIN categories c ON b.category_id = c.id
                WHERE b.search_vector @@ plainto_tsquery('korean', %s)
                AND b.status = 'active'
                ORDER BY rank DESC, b.view_count DESC
                LIMIT %s OFFSET %s
            """
            
            offset = (page - 1) * per_page
            cursor.execute(search_query, [query, query, per_page, offset])
            
            search_results = [dict(row) for row in cursor.fetchall()]
            
            # ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸
            for result in search_results:
                self._increment_search_count(result['id'], query)
            
            response_data = {
                'status': 'success',
                'data': {
                    'results': search_results,
                    'query': query,
                    'pagination': {
                        'page': page,
                        'per_page': per_page,
                        'total': len(search_results)  # ì •í™•í•œ countëŠ” ë³„ë„ ì¿¼ë¦¬ í•„ìš”
                    }
                },
                'meta': {
                    'search_time': '0.05s',  # ì‹¤ì œ ê²€ìƒ‰ ì‹œê°„ ì¸¡ì • í•„ìš”
                    'generated_at': datetime.utcnow().isoformat()
                }
            }
            
            return jsonify(response_data)
            
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': 'ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                'error': str(e)
            }), 500
    
    def _increment_view_count(self, business_id: str):
        """ì¡°íšŒìˆ˜ ì¦ê°€ (ë¹„ë™ê¸° ì²˜ë¦¬)"""
        cursor = self.db.cursor()
        cursor.execute("SELECT increment_business_view(%s)", (business_id,))
        self.db.commit()
    
    def _increment_search_count(self, business_id: str, search_term: str):
        """ê²€ìƒ‰ìˆ˜ ì¦ê°€ (ë¹„ë™ê¸° ì²˜ë¦¬)"""
        cursor = self.db.cursor()
        cursor.execute("SELECT increment_business_search(%s, %s)", (business_id, search_term))
        self.db.commit()

# í†µê³„ API
@app.route('/api/v1/statistics/hotplaces', methods=['GET'])
def get_hotplace_statistics():
    """í•«í”Œë ˆì´ìŠ¤ë³„ í†µê³„"""
    try:
        cursor = db.cursor()
        cursor.execute("SELECT * FROM hotplace_stats ORDER BY total_businesses DESC")
        
        stats = [dict(row) for row in cursor.fetchall()]
        
        return jsonify({
            'status': 'success',
            'data': {
                'hotplace_stats': stats
            }
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': 'í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
            'error': str(e)
        }), 500

@app.route('/api/v1/statistics/categories', methods=['GET'])
def get_category_statistics():
    """ì¹´í…Œê³ ë¦¬ë³„ í†µê³„"""
    try:
        cursor = db.cursor()
        cursor.execute("SELECT * FROM category_stats ORDER BY total_businesses DESC")
        
        stats = [dict(row) for row in cursor.fetchall()]
        
        return jsonify({
            'status': 'success',
            'data': {
                'category_stats': stats
            }
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': 'í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
            'error': str(e)
        }), 500
```

### 7.2 GraphQL API (ê³ ê¸‰ ì¿¼ë¦¬ ì§€ì›)

```python
# graphql_schema.py
import graphene
from graphene_sqlalchemy import SQLAlchemyObjectType
from graphql import GraphQLError

class BusinessType(SQLAlchemyObjectType):
    class Meta:
        model = Business
        interfaces = (graphene.relay.Node,)
    
    # ê³„ì‚°ëœ í•„ë“œë“¤
    computed_status = graphene.String()
    is_new_business = graphene.Boolean()
    quality_grade = graphene.String()
    
    def resolve_computed_status(self, info):
        if self.status == 'closed' or self.close_date:
            return 'closed'
        elif self.status == 'temporary_closed':
            return 'temporary_closed'
        return 'active'
    
    def resolve_is_new_business(self, info):
        if self.open_date:
            return (datetime.now().date() - self.open_date).days <= 90
        return False
    
    def resolve_quality_grade(self, info):
        score = self.data_quality_score or 0
        if score >= 90:
            return 'A'
        elif score >= 80:
            return 'B'
        elif score >= 70:
            return 'C'
        else:
            return 'D'

class Query(graphene.ObjectType):
    # ì—…ì²´ ê´€ë ¨ ì¿¼ë¦¬
    business = graphene.Field(BusinessType, id=graphene.ID(required=True))
    businesses = graphene.List(
        BusinessType,
        hotplace=graphene.String(),
        category=graphene.String(),
        status=graphene.String(),
        search=graphene.String(),
        limit=graphene.Int(default_value=20),
        offset=graphene.Int(default_value=0)
    )
    
    # ê²€ìƒ‰ ì¿¼ë¦¬
    search_businesses = graphene.List(
        BusinessType,
        query=graphene.String(required=True),
        limit=graphene.Int(default_value=10)
    )
    
    # í†µê³„ ì¿¼ë¦¬
    hotplace_statistics = graphene.List(HotplaceStatsType)
    category_statistics = graphene.List(CategoryStatsType)
    
    def resolve_business(self, info, id):
        query = BusinessType.get_query(info)
        return query.filter(Business.id == id).first()
    
    def resolve_businesses(self, info, **args):
        query = BusinessType.get_query(info)
        
        # í•„í„° ì ìš©
        if args.get('hotplace'):
            query = query.join(Hotplace).filter(Hotplace.name == args['hotplace'])
        
        if args.get('category'):
            query = query.join(Category).filter(Category.name == args['category'])
        
        if args.get('status'):
            query = query.filter(Business.status == args['status'])
        
        if args.get('search'):
            search_term = args['search']
            query = query.filter(
                Business.search_vector.match(search_term)
            )
        
        # í˜ì´ì§€ë„¤ì´ì…˜
        offset = args.get('offset', 0)
        limit = args.get('limit', 20)
        
        return query.offset(offset).limit(limit).all()
    
    def resolve_search_businesses(self, info, query, limit=10):
        """ê³ ê¸‰ ì „ë¬¸ê²€ìƒ‰"""
        businesses = BusinessType.get_query(info).filter(
            Business.search_vector.match(query),
            Business.status == 'active'
        ).order_by(
            Business.view_count.desc()
        ).limit(limit).all()
        
        # ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸ (ë¹„ë™ê¸°)
        for business in businesses:
            increment_search_count.delay(business.id, query)
        
        return businesses

# GraphQL ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
schema = graphene.Schema(query=Query)

@app.route('/api/graphql', methods=['POST'])
def graphql_endpoint():
    data = request.get_json()
    
    try:
        result = schema.execute(
            data.get('query'),
            variables=data.get('variables'),
            context={'request': request}
        )
        
        return jsonify({
            'data': result.data,
            'errors': [str(error) for error in result.errors] if result.errors else None
        })
    
    except Exception as e:
        return jsonify({
            'errors': [str(e)]
        }), 400

# GraphQL ì¿¼ë¦¬ ì˜ˆì‹œ ë¬¸ì„œ
GRAPHQL_EXAMPLES = """
# ì—…ì²´ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
{
  business(id: "uuid-here") {
    id
    name
    address_detail
    computedStatus
    isNewBusiness
    qualityGrade
    hotplace {
      name
      displayName
    }
    category {
      name
      displayName
    }
  }
}

# ì¡°ê±´ë¶€ ì—…ì²´ ëª©ë¡ ì¡°íšŒ
{
  businesses(hotplace: "ì„œë©´", category: "ì‹ì‚¬ìˆ ", limit: 10) {
    id
    name
    thumbnail
    viewCount
    favoriteCount
    hotplace {
      name
    }
    category {
      name
    }
  }
}

# ê²€ìƒ‰ ì¿¼ë¦¬
{
  searchBusinesses(query: "ë§›ì§‘", limit: 5) {
    id
    name
    address_detail
    viewCount
    qualityGrade
  }
}

# í†µê³„ ì •ë³´ ì¡°íšŒ
{
  hotplaceStatistics {
    name
    totalBusinesses
    activeBusinesses
    newBusinesses3m
  }
  
  categoryStatistics {
    name
    totalBusinesses
    totalViews
    totalFavorites
  }
}
"""
```

### 7.3 SDK ë° í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# ocler_sdk.py
"""
ì˜¤í´ëŸ¬ í”Œë«í¼ Python SDK
ê°œë°œíŒ€ì´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
"""

import requests
import json
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Business:
    id: str
    name: str
    hotplace_name: str
    category_name: str
    address_detail: str
    status: str
    view_count: int = 0
    favorite_count: int = 0
    thumbnail: Optional[str] = None
    map_link: Optional[str] = None
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []

@dataclass 
class SearchResult:
    businesses: List[Business]
    total: int
    page: int
    per_page: int
    query: Optional[str] = None

class OclerClient:
    """ì˜¤í´ëŸ¬ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.ocler.com"):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
            'User-Agent': 'Ocler-Python-SDK/1.0'
        })
    
    def get_businesses(self, 
                      hotplace: Optional[str] = None,
                      category: Optional[str] = None,
                      status: str = 'active',
                      page: int = 1,
                      per_page: int = 20,
                      sort: str = 'name',
                      order: str = 'asc') -> SearchResult:
        """ì—…ì²´ ëª©ë¡ ì¡°íšŒ"""
        
        params = {
            'page': page,
            'per_page': per_page,
            'status': status,
            'sort': sort,
            'order': order
        }
        
        if hotplace:
            params['hotplace'] = hotplace
        if category:
            params['category'] = category
        
        response = self._make_request('GET', '/api/v1/businesses', params=params)
        
        businesses = [
            Business(**business_data) 
            for business_data in response['data']['businesses']
        ]
        
        pagination = response['data']['pagination']
        
        return SearchResult(
            businesses=businesses,
            total=pagination['total'],
            page=pagination['page'],
            per_page=pagination['per_page']
        )
    
    def get_business(self, business_id: str) -> Business:
        """ì—…ì²´ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        
        response = self._make_request('GET', f'/api/v1/businesses/{business_id}')
        business_data = response['data']['business']
        
        return Business(**business_data)
    
    def search_businesses(self, 
                         query: str,
                         page: int = 1,
                         per_page: int = 20) -> SearchResult:
        """ì—…ì²´ ê²€ìƒ‰"""
        
        params = {
            'q': query,
            'page': page,
            'per_page': per_page
        }
        
        response = self._make_request('GET', '/api/v1/search', params=params)
        
        businesses = [
            Business(**business_data)
            for business_data in response['data']['results']
        ]
        
        pagination = response['data']['pagination']
        
        return SearchResult(
            businesses=businesses,
            total=pagination['total'],
            page=pagination['page'],
            per_page=pagination['per_page'],
            query=query
        )
    
    def get_hotplace_statistics(self) -> List[Dict]:
        """í•«í”Œë ˆì´ìŠ¤ë³„ í†µê³„"""
        response = self._make_request('GET', '/api/v1/statistics/hotplaces')
        return response['data']['hotplace_stats']
    
    def get_category_statistics(self) -> List[Dict]:
        """ì¹´í…Œê³ ë¦¬ë³„ í†µê³„"""
        response = self._make_request('GET', '/api/v1/statistics/categories')
        return response['data']['category_stats']
    
    def _make_request(self, method: str, endpoint: str, **kwargs) -> Dict:
        """API ìš”ì²­ ì‹¤í–‰"""
        url = f"{self.base_url}{endpoint}"
        
        try:
            response = self.session.request(method, url, **kwargs)
            response.raise_for_status()
            
            return response.json()
            
        except requests.exceptions.HTTPError as e:
            if response.status_code == 401:
                raise OclerAuthenticationError("ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ì…ë‹ˆë‹¤.")
            elif response.status_code == 403:
                raise OclerPermissionError("ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
            elif response.status_code == 404:
                raise OclerNotFoundError("ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            elif response.status_code == 429:
                raise OclerRateLimitError("ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
            else:
                raise OclerAPIError(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")
                
        except requests.exceptions.RequestException as e:
            raise OclerConnectionError(f"ì—°ê²° ì˜¤ë¥˜: {e}")

# ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ë“¤
class OclerError(Exception):
    """Base exception for Ocler SDK"""
    pass

class OclerAuthenticationError(OclerError):
    """Authentication failed"""
    pass

class OclerPermissionError(OclerError):
    """Permission denied"""
    pass

class OclerNotFoundError(OclerError):
    """Resource not found"""
    pass

class OclerRateLimitError(OclerError):
    """Rate limit exceeded"""
    pass

class OclerAPIError(OclerError):
    """API error"""
    pass

class OclerConnectionError(OclerError):
    """Connection error"""
    pass

# í¸ì˜ í•¨ìˆ˜ë“¤
def get_businesses_by_hotplace(api_key: str, hotplace: str) -> List[Business]:
    """íŠ¹ì • í•«í”Œë ˆì´ìŠ¤ì˜ ì—…ì²´ ëª©ë¡ ì¡°íšŒ"""
    client = OclerClient(api_key)
    result = client.get_businesses(hotplace=hotplace)
    return result.businesses

def search_nearby_businesses(api_key: str, query: str, limit: int = 10) -> List[Business]:
    """ì£¼ë³€ ì—…ì²´ ê²€ìƒ‰"""
    client = OclerClient(api_key)
    result = client.search_businesses(query, per_page=limit)
    return result.businesses

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # SDK ì´ˆê¸°í™”
    client = OclerClient(api_key="your-api-key-here")
    
    try:
        # ì„œë©´ ì§€ì—­ ë§›ì§‘ ê²€ìƒ‰
        restaurants = client.get_businesses(
            hotplace="ì„œë©´",
            category="ì‹ì‚¬ìˆ ",
            per_page=10
        )
        
        print(f"ì„œë©´ ë§›ì§‘ {len(restaurants.businesses)}ê°œ ë°œê²¬:")
        for business in restaurants.businesses:
            print(f"- {business.name} ({business.hotplace_name})")
        
        # íŠ¹ì • ì—…ì²´ ìƒì„¸ ì •ë³´
        if restaurants.businesses:
            first_business = client.get_business(restaurants.businesses[0].id)
            print(f"\nìƒì„¸ ì •ë³´: {first_business.name}")
            print(f"ì£¼ì†Œ: {first_business.address_detail}")
            print(f"ì¡°íšŒìˆ˜: {first_business.view_count}")
        
        # ê²€ìƒ‰ ê¸°ëŠ¥
        search_results = client.search_businesses("íŒŒìŠ¤íƒ€")
        print(f"\n'íŒŒìŠ¤íƒ€' ê²€ìƒ‰ ê²°ê³¼: {len(search_results.businesses)}ê°œ")
        
        # í†µê³„ ì •ë³´
        hotplace_stats = client.get_hotplace_statistics()
        print(f"\ní•«í”Œë ˆì´ìŠ¤ í†µê³„:")
        for stat in hotplace_stats[:5]:  # ìƒìœ„ 5ê°œë§Œ
            print(f"- {stat['name']}: {stat['total_businesses']}ê°œ ì—…ì²´")
            
    except OclerError as e:
        print(f"ì˜¤í´ëŸ¬ API ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
```

### 7.4 JavaScript/TypeScript SDK

```typescript
// ocler-sdk.ts
/**
 * ì˜¤í´ëŸ¬ í”Œë«í¼ TypeScript/JavaScript SDK
 * í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œíŒ€ì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
 */

interface Business {
  id: string;
  name: string;
  hotplace_name: string;
  category_name: string;
  address_detail: string;
  status: string;
  view_count: number;
  favorite_count: number;
  thumbnail?: string;
  map_link?: string;
  keywords: string[];
}

interface SearchResult<T> {
  data: T[];
  pagination: {
    page: number;
    per_page: number;
    total: number;
    pages: number;
  };
  query?: string;
}

interface ApiResponse<T> {
  status: 'success' | 'error';
  data: T;
  message?: string;
  meta?: {
    generated_at: string;
    cache_ttl?: number;
  };
}

export class OclerClient {
  private apiKey: string;
  private baseUrl: string;
  private defaultHeaders: Record<string, string>;

  constructor(apiKey: string, baseUrl: string = 'https://api.ocler.com') {
    this.apiKey = apiKey;
    this.baseUrl = baseUrl.replace(/\/$/, '');
    this.defaultHeaders = {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
      'User-Agent': 'Ocler-JS-SDK/1.0'
    };
  }

  /**
   * ì—…ì²´ ëª©ë¡ ì¡°íšŒ
   */
  async getBusinesses(options: {
    hotplace?: string;
    category?: string;
    status?: string;
    page?: number;
    per_page?: number;
    sort?: string;
    order?: 'asc' | 'desc';
  } = {}): Promise<SearchResult<Business>> {
    const params = new URLSearchParams();
    
    Object.entries(options).forEach(([key, value]) => {
      if (value !== undefined) {
        params.append(key, value.toString());
      }
    });

    const response = await this.makeRequest<SearchResult<Business>>(
      'GET',
      '/api/v1/businesses',
      { params }
    );

    return response.data;
  }

  /**
   * ì—…ì²´ ìƒì„¸ ì •ë³´ ì¡°íšŒ
   */
  async getBusiness(businessId: string): Promise<Business> {
    const response = await this.makeRequest<{ business: Business }>(
      'GET',
      `/api/v1/businesses/${businessId}`
    );

    return response.data.business;
  }

  /**
   * ì—…ì²´ ê²€ìƒ‰
   */
  async searchBusinesses(
    query: string,
    options: {
      page?: number;
      per_page?: number;
    } = {}
  ): Promise<SearchResult<Business>> {
    const params = new URLSearchParams({
      q: query,
      page: (options.page || 1).toString(),
      per_page: (options.per_page || 20).toString()
    });

    const response = await this.makeRequest<{ results: Business[]; pagination: any; query: string }>(
      'GET',
      '/api/v1/search',
      { params }
    );

    return {
      data: response.data.results,
      pagination: response.data.pagination,
      query: response.data.query
    };
  }

  /**
   * í•«í”Œë ˆì´ìŠ¤ë³„ í†µê³„
   */
  async getHotplaceStatistics(): Promise<any[]> {
    const response = await this.makeRequest<{ hotplace_stats: any[] }>(
      'GET',
      '/api/v1/statistics/hotplaces'
    );

    return response.data.hotplace_stats;
  }

  /**
   * ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
   */
  async getCategoryStatistics(): Promise<any[]> {
    const response = await this.makeRequest<{ category_stats: any[] }>(
      'GET',
      '/api/v1/statistics/categories'
    );

    return response.data.category_stats;
  }

  /**
   * GraphQL ì¿¼ë¦¬ ì‹¤í–‰
   */
  async graphql<T = any>(query: string, variables?: Record<string, any>): Promise<T> {
    const response = await this.makeRequest<T>(
      'POST',
      '/api/graphql',
      {
        body: JSON.stringify({ query, variables })
      }
    );

    return response.data;
  }

  /**
   * HTTP ìš”ì²­ ì‹¤í–‰
   */
  private async makeRequest<T>(
    method: string,
    endpoint: string,
    options: {
      params?: URLSearchParams;
      body?: string;
      headers?: Record<string, string>;
    } = {}
  ): Promise<ApiResponse<T>> {
    const url = new URL(`${this.baseUrl}${endpoint}`);
    
    if (options.params) {
      url.search = options.params.toString();
    }

    const headers = {
      ...this.defaultHeaders,
      ...options.headers
    };

    try {
      const response = await fetch(url.toString(), {
        method,
        headers,
        body: options.body
      });

      if (!response.ok) {
        await this.handleErrorResponse(response);
      }

      const data = await response.json();
      return data;

    } catch (error) {
      if (error instanceof OclerError) {
        throw error;
      }
      throw new OclerConnectionError(`Network error: ${error}`);
    }
  }

  /**
   * ì—ëŸ¬ ì‘ë‹µ ì²˜ë¦¬
   */
  private async handleErrorResponse(response: Response): Promise<never> {
    const errorData = await response.json().catch(() => ({}));
    const message = errorData.message || `HTTP ${response.status} Error`;

    switch (response.status) {
      case 401:
        throw new OclerAuthenticationError(message);
      case 403:
        throw new OclerPermissionError(message);
      case 404:
        throw new OclerNotFoundError(message);
      case 429:
        throw new OclerRateLimitError(message);
      default:
        throw new OclerAPIError(message);
    }
  }
}

// ì‚¬ìš©ì ì •ì˜ ì—ëŸ¬ í´ë˜ìŠ¤ë“¤
export class OclerError extends Error {
  constructor(message: string) {
    super(message);
    this.name = this.constructor.name;
  }
}

export class OclerAuthenticationError extends OclerError {}
export class OclerPermissionError extends OclerError {}
export class OclerNotFoundError extends OclerError {}
export class OclerRateLimitError extends OclerError {}
export class OclerAPIError extends OclerError {}
export class OclerConnectionError extends OclerError {}

// í¸ì˜ í•¨ìˆ˜ë“¤
export const createOclerClient = (apiKey: string, baseUrl?: string) => {
  return new OclerClient(apiKey, baseUrl);
};

export const searchBusinesses = async (apiKey: string, query: string, limit = 10) => {
  const client = new OclerClient(apiKey);
  const result = await client.searchBusinesses(query, { per_page: limit });
  return result.data;
};

// React Hook ì˜ˆì‹œ (React ì‚¬ìš©ì‹œ)
export const useBusinesses = (
  apiKey: string,
  options: {
    hotplace?: string;
    category?: string;
    page?: number;
  } = {}
) => {
  const [businesses, setBusinesses] = useState<Business[]>([]);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    const fetchBusinesses = async () => {
      setLoading(true);
      setError(null);

      try {
        const client = new OclerClient(apiKey);
        const result = await client.getBusinesses(options);
        setBusinesses(result.data);
      } catch (err) {
        setError(err instanceof Error ? err.message : 'Unknown error');
      } finally {
        setLoading(false);
      }
    };

    fetchBusinesses();
  }, [apiKey, JSON.stringify(options)]);

  return { businesses, loading, error };
};

// ì‚¬ìš© ì˜ˆì‹œ
/*
// ê¸°ë³¸ ì‚¬ìš©ë²•
const client = new OclerClient('your-api-key');

// ì—…ì²´ ëª©ë¡ ì¡°íšŒ
const businesses = await client.getBusinesses({
  hotplace: 'ì„œë©´',
  category: 'ì‹ì‚¬ìˆ ',
  page: 1
});

// ê²€ìƒ‰
const searchResults = await client.searchBusinesses('íŒŒìŠ¤íƒ€');

// GraphQL ì¿¼ë¦¬
const graphqlResult = await client.graphql(`
  {
    businesses(hotplace: "ì„œë©´", limit: 5) {
      id
      name
      viewCount
    }
  }
`);

// Reactì—ì„œ ì‚¬ìš©
function BusinessList() {
  const { businesses, loading, error } = useBusinesses('your-api-key', {
    hotplace: 'ì„œë©´'
  });

  if (loading) return <div>Loading...</div>;
  if (error) return <div>Error: {error}</div>;

  return (
    <ul>
      {businesses.map(business => (
        <li key={business.id}>{business.name}</li>
      ))}
    </ul>
  );
}
*/
```

---

## ğŸ“Š êµ¬í˜„ ë¡œë“œë§µ ë° ìš°ì„ ìˆœìœ„

### Phase 1: ê¸°ë°˜ êµ¬ì¶• (1-2ì£¼)
1. **ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ êµ¬ì¶•**
   - PostgreSQL ì„¤ì¹˜ ë° ì„¤ì •
   - í…Œì´ë¸” ìƒì„± ë° ì¸ë±ìŠ¤ ìµœì í™”
   - Row Level Security ì„¤ì •

2. **ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜**
   - CSV ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ê°œë°œ
   - ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ êµ¬ì¶•
   - ë°ì´í„° í’ˆì§ˆ ê²€ì¦

### Phase 2: API ê°œë°œ (2-3ì£¼)
1. **RESTful API êµ¬ì¶•**
   - ê¸°ë³¸ CRUD ì—”ë“œí¬ì¸íŠ¸
   - ê²€ìƒ‰ ë° í•„í„°ë§ ê¸°ëŠ¥
   - í˜ì´ì§€ë„¤ì´ì…˜ ë° ìºì‹±

2. **ë³´ì•ˆ ë° ì¸ì¦**
   - JWT ê¸°ë°˜ ì¸ì¦ ì‹œìŠ¤í…œ
   - API í‚¤ ê´€ë¦¬
   - Rate limiting êµ¬í˜„

### Phase 3: ê³ ê¸‰ ê¸°ëŠ¥ (2-3ì£¼)
1. **GraphQL API êµ¬ì¶•**
   - ìŠ¤í‚¤ë§ˆ ì •ì˜ ë° ë¦¬ì¡¸ë²„ ê°œë°œ
   - í´ë¼ì´ì–¸íŠ¸ ì¿¼ë¦¬ ìµœì í™”

2. **SDK ê°œë°œ**
   - Python SDK ê°œë°œ
   - JavaScript/TypeScript SDK ê°œë°œ
   - ë¬¸ì„œí™” ë° ì˜ˆì‹œ

### Phase 4: ìš´ì˜ ë„êµ¬ (1-2ì£¼)
1. **ëª¨ë‹ˆí„°ë§ ë° ë°±ì—…**
   - ìë™í™”ëœ ë°±ì—… ì‹œìŠ¤í…œ
   - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë„êµ¬
   - ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ

2. **ë°ì´í„° ê´€ë¦¬ ë„êµ¬**
   - ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ
   - ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
   - ìë™ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ

---

## ğŸ“ˆ ì„±ê³µ ì§€í‘œ ë° KPI

### ê¸°ìˆ ì  ì§€í‘œ
- **ì‘ë‹µ ì‹œê°„**: API í‰ê·  ì‘ë‹µì‹œê°„ < 200ms
- **ê°€ìš©ì„±**: ì‹œìŠ¤í…œ ì—…íƒ€ì„ > 99.5%
- **ë°ì´í„° í’ˆì§ˆ**: í‰ê·  í’ˆì§ˆ ì ìˆ˜ > 80ì 
- **ê²€ìƒ‰ ì •í™•ë„**: ê²€ìƒ‰ ê²°ê³¼ ë§Œì¡±ë„ > 90%

### ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ
- **API ì‚¬ìš©ëŸ‰**: ì›” API í˜¸ì¶œ ìˆ˜
- **ë°ì´í„° ì—…ë°ì´íŠ¸ ì£¼ê¸°**: í‰ê·  ë°ì´í„° ê°±ì‹  ì£¼ê¸°
- **ê°œë°œíŒ€ ë§Œì¡±ë„**: SDK ì‚¬ìš©ì„± í‰ê°€
- **ì‹œìŠ¤í…œ ì•ˆì •ì„±**: ì¥ì•  ë°œìƒ ë¹ˆë„ ë° ë³µêµ¬ ì‹œê°„

---

## ğŸš€ ê²°ë¡ 

ë³¸ ë°ì´í„° ê´€ë¦¬ ê³„íšì„œëŠ” ì˜¤í´ëŸ¬ í”„ë¡œì íŠ¸ì˜ 12,000ê°œ ì—…ì²´ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  í™œìš©í•˜ê¸° ìœ„í•œ ì¢…í•©ì ì¸ ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.

**í•µì‹¬ ì„±ê³¼**:
1. **í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°**: í–¥í›„ ë°ì´í„° ì¦ê°€ì— ëŒ€ë¹„í•œ ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ
2. **ìë™í™”ëœ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬**: ì‹¤ì‹œê°„ ê²€ì¦ ë° í’ˆì§ˆ ì ìˆ˜ ì‹œìŠ¤í…œ
3. **ê°œë°œì ì¹œí™”ì  API**: RESTful, GraphQL, SDK ì œê³µìœ¼ë¡œ ë‹¤ì–‘í•œ ê°œë°œ ìš”êµ¬ ì§€ì›
4. **ê°•ë ¥í•œ ë³´ì•ˆ ë° ë°±ì—…**: ë°ì´í„° ë³´í˜¸ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ì†ì„± ë³´ì¥

**íŒ€ì¥í‚´ê³¼ ê°•ë””ë¹„ì˜ ê³µë™ ì œì•ˆ**: ë‹¨ê³„ì  êµ¬í˜„ì„ í†µí•´ ì•ˆì •ì„±ì„ í™•ë³´í•˜ë©´ì„œë„ ê°œë°œíŒ€ì˜ ìƒì‚°ì„±ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì ì¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤.

ì´ ê³„íšì„œì˜ ëª¨ë“  ì½”ë“œì™€ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ì˜¤í´ëŸ¬ ì•±ì˜ ì„±ê³µì ì¸ ëŸ°ì¹­ê³¼ ì§€ì†ì ì¸ ì„±ì¥ì„ ì§€ì›í•  ê²ƒì…ë‹ˆë‹¤.